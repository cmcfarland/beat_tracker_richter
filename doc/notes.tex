\documentclass{scrartcl}
\usepackage[]{geometry}
% \usepackage[parfill]{parskip}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
    
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{chronology}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor=black,
    citecolor=black,
    urlcolor=black,
    linktoc=all,
    }

\begin{document}
\pagestyle{plain}


\tableofcontents
\newpage


\section{Introduction}

\begin{itemize}
\item A fundamental research topic in music information retrieval is the automatic extraction of beat locations from music signals. 
\item The aim of a beat tracker is to recover a sequence of time instants from a musical input that are consitent with the times when a human might tap their foot. \cite{Ellis2007}
\item Beat tracking is an important initioal step in computer emulation of human musc understanding, since beats are fundamental to the perception of (Western) music.
\item The goal of beat tracking is to construct a computational algorithm capable of extracting a symbolic representation which correspond to the phenomenal experience of beat or pulse in a human listener.
\item The primary information required for beat tracking is the onset times of musical events, i.e., musical notes and percussive sounds, and this is sufficient for music of low complexity and litle variation in tempo.
\item The interpretation of beat is on of the most fundamental aspects of musical intelligence.
\item A naive approach to describe the rhythm of musical data is to specify an exhaustive and accurate list of onset times, maybe together with some other musical features characterising those event, e.g., durations, pitches and intensitues. However, such a representations lacks abstraction. Diverse media used for rhyth transmission suffer a trade-off between the level of abstraction and the comprehensiveness of the representation. Standard (Western) music notation provites an accepted method for communication a composition to a performer, but it has little vale in representing the interpretation of a work as played in a caonter. On the other hand, an acoustic signal implicitly contains all rhythmic aspects but provides no abstraction whatsoever \cite{Gouyon2005}.
\item The main goal in automatic rhythm desciption is the parsing of acoustic events that occur in time into the more abstract notions of metrical structure, tempo and timing. A major difficulty is the inherent ambiguity of rhythm, as discussed in section \ref{sec:difficulties}.
\item How well does the proposed model gneralise to different musical styles?
\item The pattern recognition task is a nontrivial problem due to the wide variability of rhythm in music. 
\item It could be tackled using handcrafted rules or heristics for distinguishing on with time instant a beat is base on the properties of the audio signal. In practice such an approach leads to a profileration of rules and of exeptions to the rules and so on, and ivariably gives poor results. Far better results can be obtained by adopting a machine learning approach in which a large set of $N$ tracks $\{\mathbf x_1, \dots, \mathbf x_N\}$ called a training set is used to tune the parameters of an adaptive model. The beat instants of the tracks are known in advance, typically by inspecting them indiviualy and hand-labelling them. So for every track $\mathbf x$ there is a target vector $\mathbf t$. The result of the machine learning algorithm can be expressed as a function $\mathbf y(\mathbf x)$ which takes a new track $\mathbf x$ as input and that generates an output vector $\mathbf y$, encoded in the same way as the target vectors. The precice form of the function $\mathbf y(\mathbf x)$ is determind during the training phase, on the basis of the training data. Once the model is trained it can then determine the identity of new tracks, which are said to comprise a test set. The ability to categorize correctly new examples thet differ from those used for training is known as generalization. In practical applications, the variability of the input vectors will be such that the trainig dta can comprise only a tiny fraction of all possible input vectors, and so generalization is a central goul in pattern recognition. 
\item Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. 
\end{itemize}




\paragraph{Difficulties of Beat Tracking}
\label{sec:difficulties}
It is difficult to reliably extract high-level rhythm related features from musical excerpt having properties such as [Quinton2016]
\begin{itemize}
\item The principal reason that beat tracjing is intrinsically difficult is that is is the problem of inferring an original beat strucutre that is not expressed explicitly. The degree of beat tracking difficulty is therefore not determided simply by the number of musical instruents performing a musical piexe it depends on how explicitly the beat structure is expressed in the piece. However, it is very difficult to measure its explicitness because it is influenced from various aspects of the songs.
\item The larger the number of syncopation, the greater the difficulty of beat tracking (quantitative measure: power-difference measure \cite{Goto1999})
\item The main reason that different tendencies with regard to the explicitness with which their beat structure is indicated.
\item There is not necessarily a specific sound that directly indicates tho position of beats. In fact, a musical beat may not directly correspond to a real sound, there may even be no signal on a beat. 
\item absence of a clear rhythmic structure (Classical music)
\item soft onsets
\item blurred note transitions (e.g. classigal music dominated by string instruments)
\item heavy syncopation
\item expressive timing (e.g. rubato playing)
\item the problem of 'octave errors' (detecting double or half time the rate of the ground truth)
\item acoustiv signals consit of sound of various kinds of instruments
\item the onsets of notes are difficult to obtain, unlike the case of MIDI signals, where there is no such problem.
\item a beat may not directly correnspond to a real sound. It is a perceptal concept that human feels in music. 
\item Multiple interpretations of beats are possible at any given time
\item There is no simple relationship beteen polyphonic complexity - the number and timbres of notes played at a single time - in apiece of music, and its rhythmic complexity or pulse complexity \cite{Scheirer1998}. There are pieces and sryles of music which are texturally and timbrally complex, but have straighforward, perceptual simple rhythms; and ther also exist music which deal in less complex textures but are more difficult to rhythmically understand an describe.
\item The complexity of grouping harmonic partials together to form notes, and determining the onst times of those.
\item For this reason Cemgil et al. \cite{Cemgil2001} define music transcription as the extraction of an \emph{acceptable} music notation.
\item Ther is no canonical form for representing rhythm, and lacking this ground truth, it is difficult, if not impossible, to provide a meaningful quantitative comparison of the variuos computer systems. Therefore we need a common database of test music labelled with the ground truth on which the systems are tested. 
\end{itemize}



% Quantify the difficulty of beat tracking with indicators such as
% \begin{itemize}
% \item beat strength [Tzanetakis2002]
% \item pulse clarity [Lartillot2008]
% \item entropy of a cyclic tempogram [Grosche2010] as an indicator of the tempo salience  
% \end{itemize}


\paragraph{Applications of Beat Tracking}

Estimating the beats of an musical audio opens new possibilities for a wide range of applications. 

\begin{itemize}
\item Beat tracking can be used to automate the time-consuming tasks that must be completed in order to synchronize events with music. 
\item Video and audio editing (visual track can be automatically synchronized with an audio track using beat tracking)
\item Stage light control. In live performances, beat tracking is useful in the control of stage lighting by a computer. For instance, various properties of lighting such as color, brightness, direction, and effect can be changed in time to the music.
\item Visualization for example time-grid in audio sequencers.
\item Musical interaction systems \cite{Robertson2007}
\item Content-based audio effects, for multimedia or interactive performances or studio post-production
\item Beat-driven reat-time computer graphics.
\item Temoral segmentation to higher level MIR tasks such as chord estimation for harmonic description \cite{Bello2005b}.
\item Long-term structural segmentaion of audio \cite{Bartsch2005}
\item performance analysis investigates the interpretation of musical works, e.g., the performer's choice of tempo and expressive timing. 
\item In audio content analysis beat tracking is important for automatic indexing and content-based retrieval of audio data, such as in multimedia databases and libraries.
\item Automatic transcription and score extraction from performance data 
\item Music similarity
\item Time-streching of audio loops
\item Beat tracking can provide computers the ability to participate intelligently in live performances in real time and join the ensemble.
\item synchronisation of a musical performance with computers or other devices
\item Commercial devices already exist that attept to extract a MIDI clock from an audio signal, indication both the tempo and the actual location of the beat. Such MIDI clocks can the be used to synchronize other devices such as drum machines or audio effects, enabling a new range of beat-synchronized audio processing.
\item Automatic playlist generation, where a computer is given the task to choose a series of audio tracks from a track database in a way similar to what a human deejay would do. The track tempo is a very important selection criterion in this context, as deejays will tend to string tracks with similar tempi back to back. Furthermore, deejays also tend to perform beat-synchronous crossfading between successive track manualy, slowing down or speeding up one of the tracks so that the beat in the two tracks line up exactly during the crossfade. This can easily be done automatically once the beats are located in the two tracks.
\end{itemize}



\newpage
% Rename to 'History of Beat Tracking'?
\section{Related Work}

This chapter is an overview of related work that forstered the development of beat tracking in musical audio. The task of automatic rhythm detection has been well established over the last thirty-five years and beat tracking algorithms have constantly imporved in performance. A chronology with the most influental work is shown in Fig. \ref{fig:history}.

\begin{figure}[htpb]
\centering
\begin{chronology}[5]{1984}{2024}{\textwidth}[\textwidth]
\event{1985}{Schloss \cite{Schloss1985}}
\event{1990}{Allen and Dannenberg \cite{Allen1990}}
\event{1994}{Goto and Muraoka \cite{Goto1994}}
\event{1998}{Scheirer \cite{Scheirer1998}}
\event{2001}{Cemgil et al. \cite{Cemgil2001}, Dixon \cite{Dixon2001}}
\event{2003}{Laroche \cite{Laroche2003}}
\event{2005}{Klapuri et al. \cite{Klapuri2005}}
\event{2007}{Davies and Plumbley \cite{Davies2007}}
\event{2009}{Peeters \cite{Peeters2009}}
\event{2011}{Böck and Schedl \cite{Boeck2011}}
\event{2014}{Böck et al. \cite{Boeck2014}}
\eventmarker{2015}{Krebs et al. \cite{Krebs2015}}
\eventlabel{2016}{Krebs et al. \cite{Krebs2015}}
\event{2019}{Davies and Böck \cite{Davies2019}}
\end{chronology}
\caption{Timeline with the history of beat tracking.}
\label{fig:history}
\end{figure}

The traditional approach of beat tracking consists of extracting features from the audio signal to obtain a feature list. These features range from note onsets, as time, duration and pitch, to frequency-based signal features and they convey predominant information relevant to rhythmic analysis. The feature extraction is usually followed by a periodicity detection stage and the estimated periodicities subsequently determine the beat times using a temporal decoding strategy. Nevertheless, the recent trend in beat tracking is a shift away from purely signal processing approaches to data-driven approaches incorporating machine learning. In the following, the approaches of worth mentioning works are represented chronologically. 

% \begin{itemize}
% \item 
% \item The recent trend in music information retrieval to shift away from purely signal processing approaches to those incorporating machine learning. 
% \item Overview: Signal processing $\rightarrow$ data-driven approaches
% \item traditional approach of precessing a list of onsets
% \item new approach use artificial neural networks to perform a frame by frame beat classification.
% \item The first analysis step is the creation of a feature list, i.e. the parsing, of filtering, of the data at hand into a sequence of features. These features range from note onset features (as time, duration and pitch), to frae-based signal features, and they convey the predominant information relevant to rhythmic analysis.
% \item The extraction of note onsets times for rhythmic analysis is ubiquitous in the litarature. An exhaustive overview of musical onset detection can be found in \cite{Bello2003}.
% \item Some systems do not focus on note onsents and their features, but refer to a data granularly of a lower lever of abstraction, namely frames. A frame is a short chunk, typically $10\,\text{ms}$ of audio, from which both time and frequency domain features can be computed. Consecutive frames are usually considered with some overlap for smoother analyses, and the analysis step, the hop size, equals the frame size minus the overlap. 
% \end{itemize}


% Should I add Chowning et al. 1984?

Schloss \cite{Schloss1985} presented one of the earliest work on automatic extraction of rhythmic contet from audio in his percussion transcription system. Onsets are detected as peaks in the slope of the amplitude envelope, where the envelope is defined to be equal to the maximum aplitude in each period of the high-pass filterd signal, and the period defined as the inverse of the lowest frequency expected to be present in the signal. The main limitation of the system is that it requires parameters to be set interactively. 

Allen and Dannenberg \cite{Allen1990} extended the musical concept of beat to include two aspects, namely period and phase. Based on that concept, they built a method that uses real-time beam search to allow the beat tracker to consider several possible stages at once. They use a credibility measure so that at any given time there is a set of active states that represent the most credible interpretations for the performance encoutered so far. However, the system's reliance on MIDI limited the input source to electronic instruments, and moreover limeted its application.

Goto and Muraoka \cite{Goto1994} introduce the first worth mentioning beat tracking system which could process music played on ensembles of a variety of instruments. However, they restricted their system to rock and pop music in which drums maintain the beats. The system leverages the fact that for a large class of popular music, a bass drum and a snare drum usually occur on the strong and weak beats, respectively. It manages multiple agents that track beats according to different stategies in order to examine multiple hypothesis in parallel. All hypotheses are gathered and the most reliable one is selected as the output. This enables the system to follow beats without losing track if them, even if some hypothesis became wrong. Assumptions were made; between 65 and 185 BPM, time-signature is 4/4, tempo stays almost constant. In following developments, Goto present a beat tracking system for both music with and without drum-sounds \cite{Goto2001}. It uses frequency-domain analysis to detect chord cahnges, which are assumed to occur in metrically strong positions. This is the fist system to demonstrate the use of high level information in directing the lower-level beat tracking process. The high level information is specific to the musical style, which is a major limitation of the system.  

Scheirer \cite{Scheirer1998} figured out from psychoacoustic demonstration on beat perception, that amplitude envelopes from a small number of broad frequency channels are sufficient information to rhythmically analyse musical content. He concludes that a rhythmic processing algorithm shoud treat frequency bands separately, combining results at the end, rather than attempting to perform beat tracking on the sum of filterbank outputs. This leads him to the use of a small number of bandpass filters and banks of parallel comb filters, which function as tuned resonators, to perform periodicity analysis. In the next processing step, the phase of the musical siganl is extracted by examining the internal state of the delays of the comb filters. Finally, the phase and the period is used to estimate the beat times as far into the future as desired. One problem with the system is that in order to track tempo changes, the system must repeatedly change its choice of filter, which implies the filters must be closely spaced to be able to smoothly track tempo variations. However, the system applies no continuity constraint when switching between filters.  

Dixon \cite{Dixon2001} processes a sequence of note onset times either extracted from an audio signal or from a symbolic representation within a multi-agent system. Likely tempo hypotheses are derived from clutering inter-onset intervals (IOI), thus encoding aspects of the metical hierarchy. The hypothesis are used to form multiple beat agents using a paradigm, where each agent has a state consiting of the period and the phase of the beat. The sequence of beat times with the best score to date is selected by the agent. The observations are only processed if they occur around the predicted beat locations, i.e., within a window whose width depends on the pulse period. The algorithm is designed to track beats in expressivey performed music.
% add Dixon 2003

% In this framework, 

% \begin{itemize}
% % \item Dixon \cite{Dixon2001} uses histograms of time intervals between similar events. He uses IOI histograms where the bins are not fixed.
% % \item Clusters of similar IOIs are given scores based on the number of elements in the cluster and the amplitudes of ther onsets.
% % \item An ajustment of the scores and cluster representative interval the favours rationally-related cluster
% % \item The computation of the period entails the loss of time localisation, and the phase has to be computed subsequently  during pulse trackking. 
% % \item Uses a fixed tolerance interval of $25\,\text{ms}$ für IOIs
% % \item Presents another multiple hypotheses search, using an agent paradigm, where each agent has a state, consiting of the period and the phase of the pulse, and a history, the sequence of beat times selected to date by the agent. The observations are only processed if they occur around the predicted beat locations, i.e., within a window whose width depends on the pulse period.
% \item Dixon et al. \cite{Dixon2003} employ tolerance intervals proportional to the IOIs, so longer IOIs allow for greater variations.
% \end{itemize} 

Cemgil et al. \cite{Cemgil2001} formulate beat tracking in a Bayesian framework where tempo and beat is modeled as a stochstic dynamical system. The system is defined with two hidden state variables, namely the period and the phase of beat. To this deterministic model, they add a Gausssian random vector whose covariance matrix models the likely tempo variations. State transitions are defined by a simple set of equations that describe how state variables evolve with time. 
Because all noises are assumed to be Gaussian and all relationships between variables are linear, the covariance matrix can be efficiently estimated by a Kalman filter. They also define the tempogram representation which includes a probability distribution over the period and phase given a list of onset. This probability distribution is proportional to the likelihood of the observed onsets under given period and phase hypotheses, wheighted by prior distribution, which is equally distributed, as they consider all tempi to be initially equiprobable. For given periods and phases, the likelihood is computed as the integral over all onsets of the product of a constant pulse track and a continous representation of the onsets. This implements the assuption that a good pulse track is one which matches all onsets well. The tempogram marginal probability distribution $p(\omega | t)$ provides a 1-D representation of periodicities resembling those aforementioned. 
%  will be estimated through a training phase.

% Temperley and Sleater \cite{Temperley1999} use dynamic programming to search the solution space of possible mappings of events to a metrical grid, where the search is guided by a set of preference rules based of GTTM.

Laroche \cite{Laroche2003} initially finds salient features like note onsets, note changes, and percussion hists by calculating the Fourier transform of an audio signal. A nonlinear monotonic compression function is applied to the amplitude spectrum, so high-frequency components are not masked by higher amplitide low-frequency components. To locate fast variations in the frequency domain contents, a first-order difference is calculated. All bins are summed together, and the result is half-wave rectified to obtain a positice energy flux signal. A least-squares approach is used to determine the best candidates for the tempo and beat locations. The final step consist of going through the successive tempo analysis frames and finding in each frame the best candidates. To that effect a dynamic programming techning is used. This entails continuity and non-sysncopation constraints. 

Klapuri et al. \cite{Klapuri2005} expand upon Scheirer's amplitude envelope and comb filter model. They adopt a more robust registral accent signal across four parallel analysis bands as the input to their system and use comb filterbanks within a probabilistic framework to simultaniously track three metrical levels. These correspond to the tatum, tactus and measure, which are explained in section \ref{sec:perception}. Analysis can be performed causally an noncausally, and is not restricted to any particular genre, tempo or time-signature. The robustness of the analysis model is due to the probabilistic modelling of the temporal evolution and interaction between each og the three metrical levels analysed. In a study of audio temoo induction algorhithms \cite{Gouyon2006b}, this approach was shown to be most accurate.

Davies and Plumbley \cite{Davies2007} adopt a simpler and more efficient, heuristic approach than the system of Klapuri by embedding context-dependent information directly into the beat period and alignment estimation processes. They use two state model; the first state performs tempo induction and tracks tempo changes, while the second maintains contextual continuity within a single tempo hypotheseis. The first state, also called general state, operates in a memoryless fashion, extracting the beat period and beat alignment through a process of repeated induction. In this manner, the two-state model can explicitly model tempo discontinuities while smoothing out odd beat errors during each consistent beat period hypothesis. 

Peeters \cite{Peeters2009} approach is based on a probabilistic framework, in which the beat tracking problem is formulated in a hidden Markov model, that can be efficiantly solved with the Viterbi algorithm \cite{Viterbi1967}. An onset-energy-function, time-variable tempo, and meter serves as an input to the system. Beat times are decoded over beat-numbers according to observation and transition probabilities. A beat-template is used to derive the observation probabilities from the signal. For this purpose, a linear discriminant analysis finds the most discriminative beat-template. 
% A significant change was introduced by Eyben who transit from signal processing to data-driven method which utilise the power of machine learning. 

Böck and Schedl \cite{Boeck2011} present the first beat tracking system which is based on artificial neural networks. The network transforms the signal directly into a beat activation function, which represents the probability of a beat at each frame.  As network architecture they use a bidirectional recurrent neural network (RNN) with long short term memory (LSMT) units. The approach is inspired by the good results for musical onset detection \cite{Eyben2010} and extended to suit the needs for audio beat tracking by modifying the input representation and adding a peak detection stage. As input to the network, three filtered magnitude spectra with different window lengths and their first order differences are used. In a peak detection stage, first the periodicity within the activation function is detected with the autocorrelation function to deternine the most dominant tempo. The beats are then aligned according to the previously computed beat interval. In this way, erroneously detected beats are eliminated or missing beats are complemented.

Böck et al. \cite{Boeck2014} extend the previous beat tracking system of Böck and Schedl with a multi-model approach to represent different music styles. For this purpose, they use multiple recurrent neural networks, which are trained on certain heterogeneous music styles. The system chooses the model with the most appropriate beat activation function for the input signal and jointly models tempo and phase of the beats with a dynamic Bayesian network. Compared to a reference model, which was trained on the whole trainig set, the specialised models produce better predictions on input data which is similar to that used for training, but worse predictions on signals dissimilar to the training data.

Krebs et al. \cite{Krebs2015} propose a modified state-space discretisation and tempo transition model for the temporal decoding stage with dynamic Bayesian networks. The modification increases beat tracking accuracy and also reduces time and memory complexity. To be consistent with human tempo sensitivity, they propose to make the number of discrete bar positions dependent on the tempo and distribute the tempo states logarithmically across the range of beat intervals. 
% See Section \ref{sec:temporal_decoding} for details.

Davies and Böck \cite{Davies2019} suggest to use a convolutional neural network in form of a temporal convolutional network (TCN) \cite{Bai2018}. In comparison to the recurrent model of Böck et al. \cite{Boeck2014}, the TCN can be trained more efficiently on very large datasets due to parallelisation. It requires a smaller number of trainable parameters while achieving state-of-the-art performance. 



\newpage
% basic principles of how humans understand rhythms
% Old: Basic Principles of Rhythm
\section{Rhythm}
\label{sec:perception}

Rhythmic organization is an inhernt part of all human activity. In this chapter, basic principles and definitions about rhythm in music are explained.  


\subsection{Terminology}

\paragraph{Rhythm} 
In music and music theory, there are many different definitions of rhythm. Generally, rhythm means a movement marked by the regulated succession of strong and weak elements, or of opposite or different conditions \cite{Dictionary1971}. It is regarded as the way in which accented and non-accented notes are grouped in a time unit \cite{Cooper1966}. The definition of Lester \cite{Lester1986} considers the patterns of duration between musical events and has the advantage that events pertaining to various musical qualities, giving rise to the idea that more than one rhythm can be defined for a musical piece. Whereas London \cite{London2001} defines rhythm as the sequential pattern of durations relatively independent of meter or phrase structure. In general, the experience of rhythm involves movement, regularity, grouping, and yet accentutation and differentiation \cite{Handel1989}.


\paragraph{Onset} 
An onset refers to the beginning of a musical note or other sound. Any rhythmic event is basically characterised by an onset time and a salience. They represent the most basic unit of rhythmic information, from which all beat and tempo information is derived. The concept of onsets is related to the concept of transients, but differs in the way that all musical notes have an onset, but do not necessarily include an initial transient. The more salient events are the more likely to correnspond to beat times than the less salient ones. This tendency for events with greater perceptual salience to occur in stronger metrical positions has been noted by various authors \cite{Lerdahl1985,Povel1985,Parncutt1994}. Lerdahl and Jackenhoff \cite{Lerdahl1985} classify musical accents into three types: phenomenal accents, which come from physical attributes of the signal such as amplitude and frequency; structural accents, which arise from perceived points of arrival and departure such as cadences; and metrical accents, points in time which are perceived as accented due to their metrical position.


\paragraph{Beat} 
The term beat, or more technically the \emph{tactus}, refers to the perceived pulses which are approximately equally spaced and define the rate at which notes in a piece of music are played \cite{Handel1989}. Intuitively, it is often defined as the rhythm listeners would tap their foot to when listening to a piece of music, or the numbers a musician counts while performing. Therefore, the beat is most often designated as a crotchet or quarter note in western notation. In beat tracking, the period of a beat is the time duration between two successive beats, i.e. the reciprocal of the tempo. Whereas the phase determines where a beat occurs with respect to performance time.
% For a specific performance, the beat is defined by the occurance times, beat times, of these pulses.
% The beat of a piece of music is the sequence of equally spaced phenomenal impulses which define a tempo for the music. 


\paragraph{Tempo}
Given a metrical structure, tempo is defined as the rate of beats at a given metrical level. Thus, it corresponds to the frequency of the primary pulse in a rhythmic musical signal. The tempo is commonly expressed as a number of beats per minute (BPM). In order to represent changing tempi, various approaches can be used. If tempo is considered as an instantaneous value, it can be calculated as the inter-beat interval (IOI) measured between each pair of successive beat. A more preceptual plausible approach is to take an average tempo measured over a longer period of time. A measure of central tendency of tempo over a complete musical excerpt is called the basic tempo, which is the implied tempo around which the expressive tempo varies \cite{Repp1994}. The value of tempo as a function of time is called a tempo curve, and can be visualized in a tempogram \cite{Cemgil2001}.


\paragraph{Meter} 
% Metrical level see Dixon 2001

The term meter refers to the regularly recurring patterns and accents such as beats and bars and provides an underlying time frame. Unlike rhythm, meter is a perceptual concept which is inducted from the phenomenally accented points of musical surface \cite{London2004}. The metrical structure is hierarchical, i.e., it involves a ratio relationship between at least two time levels, namely the referent time level, the beat, and a higher order period based on a fixed number of beat periods, the measure \cite{Yeston1976}. Meter is regular and stable, and serves as a kind of enhanced temporal grid, which helps to shape expectations about the future and thus be able to anticipate and predict events in time \cite{Huron2006}. In order to establish a meter, some regularity has to be manifesed in the acoustic signal in the first place. Once meter has established, all other events are perceived with refernce to this regular pattern.  In Lerdahl and Jackendoff's \emph{A Generative Theory of Tonal Music} (GTTM) \cite{Lerdahl1985}, the rhythmic structure in the tonal music of the Western tradtition consits of two independent elements, grouping and meter. Grouping is the manner in which music is segmented at a whole variety of levels from groups of a few notes up to large-scale from of a piece of music. Whereas meter is described as the regular alteration of strong and waek elements in music. The metrical structure deals with durationless points in time, e.g. the beats, which obey some well-defined rules. In the GTTM, meter perception is described as the progress of finding periodicities in the phenomenal and structural accents in a piece of music. It also propose a set of metrical preference rules, based on musical intuitions, which are assumed to guide the listener to plausible interpretations of rhythms. Nonetheless, a major weakness of the GTTM is that it does not deal with the departures from strict metrical timing which are apparent in almost all styles of music. Thus, it is only suitable for representing the timing structures of musical scores, or as an abstract representation of a performance, where expressive timing is not represented.

% Tatum, tactus, measure

% They formalise the distinction objective stucture and the performers conceptualisation of music by defining rules for a ``musical grammar'' which deals separately with grouping structure and metrical structure. While the grouping structure deals with time spans, 

% Beats must be equally spaced. A division according to a specific duration corresponds to a metrical level. There must be a beat of the the metrical structure for every note in a musical sequence. A beat at a high level must also be a beat at each lower level. At any metrical level, a beat which is also abeat at the next higher level is called a downbeat, and other beats are called upbeats. Beats obey a discrete time grid, with time intervals all being multiples of a common duration the smalles metrical level. 

% The rules prefer structures where: beats coincide with note onsets; string beats coincide with onsets of long notes; parallel groups receive parallel metrical structure; and the strogest beat occurs early in the group

% Western music notation provides an objective regular temporal structure underlying musical event occurances and organising them into a hierachical metrical structure \cite{Gouyon2005}. 

% Although meter is visualized as a train of pulses with differnet salience, or durationless events in time which conform to a highly regular pattern of hierarchical accentuation, the onsets of actual musical events does not have to coincide with the ponts in time where those pulses are situated. 

% As contrasted with rhythm in general, beat and and pulse correspond inly to the sense of equally spaced temporal units \cite{Handel1989}. Where meter and rhythm associate with qualities of grouping, hierarchy and a strong/weak dichitimy, pulses in apiece of music are only periodic at a simple level.


% old: Psychoacoustics and Human Perception
\subsection{Rhythm Perception}

More psychologically or cognitively motivated definitions associate rhythm to the perceived patterns generated by recurring events and how they interact and categorized by listeners. 

Perception of beat is a prerequisite to rhythm perception, which in turn is a fundamental part of music perception.


There is no ground thuth for rhythm to be found in simple measurement of an acoustic signal. The only ground truth is what human listeners agree to be the rhythmic aspects of the musical contetent of that signal

The perception of tempo exhibits a degree of variability. It is not always correct to assume that the denominator of the time signature corresponds to the ``foot-tapping'' rate, nor to the actual ``physical tempo'' that would be an inherent property of audio flows \cite{Drake1999}.

Studies of Povel and Essnes \cite{Povel1985} have demonstrated that beat perception may be explianed with a model in which a perceptual clock is aligned with the accent structure of the input. The model relies heavily on structural qualities of the input, such as a sophisticated model of temporal accent, to function. They propose a model of perception of temporal patterns, based on the idea that a listener tries to induce an internal clock which matches the distribution of accents in the stimulis and allows the pattern to be expressed in the simplest possible terms. They use patterns of identical tone bursts at precise multiples of $200\, \text{ms}$ apart to test their theory. 


Each pair of events in a rhythmic sequence initially contributes to the salience of a single pulse sensation (emphasis occurs), and later that pulse sensations can enhance the salience of other consonant pulse sensations \cite{Parncutt1994}. One may understand the initially above as an indication not to implement influential schemes beteen metrical levels in the induction process, but imdeed to do it in the tracjing process, which is also in agreement with the Dynamic Attending Theory \cite{Jones1989}. 

Drake and Bertrand \cite{Drake2001} advocate a universal predisposition toward simple duration ratio, and claim that humans thend to hear a time interval as twice as long or short as previous intervals. The Dynamic Attending Theory \cite{Jones1989} proposes that humans spontaneously focus on a reference level of periodicity, and they can later switch to other levels to track events occuring at differnt time spans, e.g., longer span harmony changes, or a particular shorter-span fast motive.

Jones et al. \cite{Jones1989} propose that perceived rhythm is the result of differnet attending modes (future-orianted and analytic) which involve anticipatory behaciors to coherent temporal events and their durational patterns. They list the following types of phenomenal accent, which they consider incomplete: note onsets, sforzandi, sudden dynamic or timbral changes, long notes, melodic leaps and harmonic changes. However, they give no indication as to how these factors might be compared or combined, either quantitatively (absolute values) or quanlitatively (relative strengths). Longer notes tend to be perceived as accented.

Human pitch recognition is only sensitive to signal phase under certain unusual conditions. Rhthmic response is crucially a phased phenomenon - tapping on the beat is not at all the same as tapping against the beat. or slightly ahead of or behind the beat, even ig the frequency of tapping is accurate \cite{Scheirer1998}

From psychoacoustic demonstration on beat perception it can be shown that certain kinds of signal manipulations and simplifications can be performed without affecting the perceived pulse content of a musical signal. An amplitude-modulated noise constructed by vocoding a white noise signal with the subband envelopes of a musical signal is sufficient to extract pulse and meter. The simplified signal is created by performing a frequency analysis of the original signal by processing it through a filterbank of bandpass filters, or grouping FFT bins together. Thus it seems that separating the signal into subbands and maintaining the subband envelopes separately is necessary to do accurate rhythmic processing. This fact leads to the hypotheses that some sort of cross-band ryhtmic integration, not simply summation across frequency bands, is performed by the auditory system to perceive rhythm \cite{Scheirer1998}.

Although in the brains of performers music is temporally organized according to its hieaechical beat structure, this structure is not explicitly expressed in music; it is implied in the relations among variuos musical elements which are not fixed and which are dependent on musical genres or poeces.

Expressive timing is generated from perforers' understanding of the musical structure an general knowledge of music theory and musical style \cite{Clarke1999}. However, there is no precise mathematical model of expressive timing, and the complexity of musical structure fro which timing is derived, coupled with the individuality of each performer and performance, makes it impossible to capture musical nuance in the form of rules \cite{Dixon2001}. 

Expressive reductions in tempo are more common and more extreme than tempo increases \cite{Repp1994}. % change transition prob in HMM

Differences in human perception of tempo depend on age, musical training, musical preferences and general listening context, e.g., tempo of a previously heard sequence, listener's activity, instant of the day \cite{Drake1993,Drake2000,Drake2000b}

Differences in tempo perception are nevertheless far from random. They most often correspond to a focus on a different metrical level, e.g., differences of half or twice the inter-beat interval or one-third or three times the inter-beat interval. 

Perception research has shown that with up to $40\,\text{ms}$ difference in onset times, two tones are heard as synchronous, and for more than two tnoes, the threshold is up to $70\,\text{ms}$ \cite{Handel1989}. Rhythmic information is provided by IOIs in the range of approcimately $50\,\text{ms}$ to $2\,\text{s}$ \cite{Handel1989}.

Experiments on the tempo sensitivity on humans have shown that the ability to notice tempo changes is proportional to the tempo, with the JND (just noticable difference) beeing around 2-5\% \cite{Drake1993}.

A listener who cannot identify chord names can nevertheless percieve chord changes.

% Many works in musicpsychology attempt to group musical onsets together into a rhythmic context, i.e., to construct a model which subsumes multiple onsets separated in time into a rhythmic clock, hierachy, grouping, or oscillatory model.

\newpage

% Old: Foundations of Machine Learning
\section{Machine Learning}

Machine learning is subset of artificial intelligence (AI) and deals with algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is essentially a form of applied statistics with increase emphasis on the use of computers to estimate comlicated functions and a devreased emphasis on providing confidence intercals around these functions \cite{Goodfellow2016}. On the other hand, pattern recognition is concerned with the automatic discovery of regularities in data, and to take actions such as classifying the data into different categories. Both fields, machine learning and pattern recognition are strongly connected with each other, and together they have undergone a substential development over the past twenty-five years \cite{Bishop2006}.

Most machine learning algorithms can be divided into the categories of \emph{supervised learning} and \emph{unsupervised learning}. Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. Supervised learning algorithms include classification and regression. Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. 

Machine learning tasks are usually described in terms of how the machine learning system should process an example $\mathbf x \in \mathbb R^n$, where an example is a collection of $n$ features $x_i$ that have been quantitatively measured from some object or event. Many kinds of tasks can be solved with machine learning, e.g. classification, regression, transcription, machine translation, synthesis and sampling just to name a few.

In this thesis, the focus is on sequence modeling and classification which can be solved with supervised learning. In the following, basic concepts of machine learning are described. 

% Old: Sequence Modeling
\subsection{Sequence Labelling}
In machine learning, the term sequence labelling encompasses all tasks where sequences of data are transcribed with sequences of discrete labels. Supervised sequence labelling refers specifically to those cases where a set of hand-transcribed sequences is provided for algorithm training. What distinguishes such problems from the traditional framework of supervised pattern classification is that the individual data points cannot be assumed to be independent. Instead, both the inputs and the labels form strongly correlated sequences \cite{Graves2012}.

Given an input sequence $\mathbf x_{1:T} = \mathbf x_1, \dots, \mathbf x_T$ with sequence length $T$, a sequence model is any function $f: \mathcal X^T \rightarrow \mathcal Y^T$ such that
\begin{align}
\mathbf y_{1:T} = \mathbf y_1, \dots, \mathbf y_T = f(\mathbf x_1,\dots, \mathbf x_T)
\end{align}
where $\mathbf y_t$ should only depend on $\mathbf x_{1:t}$ and not on $\mathbf x_{t+1:T}$, i.e., no leakage of information from the future. This causality constraint is essential for autoregressive modeling. We use $\mathbf x_{1:T} = \mathbf x_1, \dots, \mathbf x_T$ to denote a input sequence of length $T$, where vector $\mathbf x_t \in \mathbb R^n$ is the input at time step $t$. Thus $\mathbf x_{1:T} \in \mathbb R^{T \times n}$.

The goal of learning in the sequence modeling setting is to find a network $f$ that minimizes some expected loss between the actual outputs and the predictions, 
\begin{align}
L \big( \mathbf y_1, \dots, \mathbf y_T, \, f(\mathbf x_1,\dots, \mathbf x_T) \big) \stackrel{\text{!}}{=} \min
\end{align}
where the sequences and outputs are drawn according some distribution.

For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. For example the sequence modeling chapter in a standard reference on deep learning is titled ``Sequence Modeling: Recurrent and Recursive Nets'' \cite{Goodfellow2016} capturing the common association of sequence modeling and recurrent architectures. Recent results indicate that convolutional architectures can outperform recurrent networks. Concluding from an empirical evaluation of generic convolutional and recurrent networks for sequence modeling, Bai et al. \cite{Bai2018} assert that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.


\begin{itemize}
\item Sequence modeling include polyphonic music modeling \cite{Oord2016}, word- and character-level language modeling.
\item settings such as auto-regressive prediction, where we try to predict some signal given its past, by setting the target output to be simply the input shifted by ome time step
\end{itemize}


\subsection{Feature Extraction}

For most practical applications, the original input variables are typically preprocessed to transform them into some new space of variables where, it is hoped the pattern recognition problem will be easier to solve. This preprocessing stage is sometimes also called feature extraction. New test data must be preprocessed using the same steps as the training data. Preprocessing might also be performed in order to speed up the computation, for example if the goal is a real-time application the alogorithm should be computationally feasible. Yet, the aim of feature selection is to find features in the input which preserve useful disciminatory information. Usually, the number of features is smaller than the number of input variables, thus a preprocessing is a form of dimensionality reduction. 

% Dimension reduction

\begin{itemize}
\item Many artificial intelligence tasks can be solved by designing the right set of features to extraxt for that task, then providing theses features to a simple machine learning algorithm. \cite{Goodfellow2016}
\end{itemize}


\subsection{Performance Measure}

To evaluate the abilities of a machine learning algorithm, we must design a quantitative measure of its performance. Usually the performance measure is specific to the task being carried out by the system. For the task of classification, the accuracy of the model is often measured. Accuracy is the proportion of examples for which the model produces the correct output. 

The choice of performance measure may seem straightforward and objective, but it is often difficult to choose a performance measure that corresponds well to the desired behaviour of the system. 

\paragraph{Loss Function}
Cross entropy is defined for two probability distributions $p$ and $q$ as
\begin{align}
H(p,q) = - \sum_{x \in \mathcal X} p(x) \log q(x).
\end{align} 
In machine learning cross entropy can be used as a loss funciton to measures the performance of a classification model. The probability $p_{i}$ is the true label (binary indicator 0 or 1), where as the distribution $q_{i}$ is the predicted value of the current model. 


% Old: Architectures
\subsection{Model Selection}

In pracial applications, we determine the values of the model's parameters, and the principle objective in doing so is usually to achieve the best predictive performance on new data. Furthermore, as well as finding the appropriate values for complexity parameters within the model, we wish to consider a range of different types of model in order to finde the best one for our particular application. 

The performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-fitting. If data is plentiful, then one approach is simply to use some of the availible data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independet data, also called the validation set, and select the one having the best predictive performance. If the model design is iterated many times using a limited dataset, then some over-fitting to the validation data an occur and so it may be necessary to keep aside a third test set on which the performance of the selceted model is finally evaluated. 


\begin{itemize}
\item Given a sequence modeling task or dataset, which architecture should one use?
\item Bai et al. \cite{Bai2018} show that a simple convolutional architectures outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. Is it also the case in beat tracking?
\end{itemize}


\subsection{Optimization}

Most machine learning algorithms involve optimization which refers to the task of minimzing an objective function $f(\mathbf x)$ by altering the arguments $\mathbf x$. The goal of optimization is to find an optimal $\mathbf x^*$, such that
\begin{align}
\mathbf x^* = \arg \min f(\mathbf x)
\end{align} 
Given the function $y=f(x)$, where both $x$ and $y$ are real numbers, the derivative $f'(x)$ is usefull for minimizing this function because it indicates how to change $x$ in order to make a small improvement in y. Thus, by moving $x$ in small steps with the opposite sign of the derivative, $f(x)$ is reduced. This technique is called gradient decent \cite{Cauchy1847}. When $f'(x)=0$, the derivative provides no more information about which direction to move and the function reaches a stationary point. Stationary points are classified into three kinds. A local minimum is a point where $f(x)$ is lower than at all neighboring points, and higher for a local maximum respectively. The third kind is known as saddle points where the derivative of the function has the same sign on both sides of the stadionary point.  

In the context of machine learning, the objective function has many local minima that are not optimal and many saddle points surrounded by very flat regions. All of this makes optimization difficult, especially when the input to the function is multidimensional. Therefore, it is usually sufficient to find  a value of $f$ that is very low but not necessarily minimal in any formal sense. Thus, in gradient decent we decrease $f$ by moving in the direction of the negative gradient to propose a new point
\begin{align}
\mathbf x' = \mathbf x - \mu \,\nabla_{\mathbf x}f(\mathbf x)
\end{align} 
where $\mu$ is the learning rate, a positive scalar determining the size of the step, and $\nabla_{\mathbf x}$ refers to the gradient with respect to the input $\mathbf x$.


\paragraph{Stochastic Gradient Decent}
Stochastic gradient descent (SGD) is an extension of the basic gradient descent algorithm. A recurring problem in machine learning is that large training sets are necessary for good generailzytion. At the same time, large learning sets are also more computationally expensive. Instead of using the whole learning set, a small set called minibatch is used to estimate the gradient
\begin{align}
\mathbf g = \frac{1}{N} \,\nabla_{\boldsymbol \theta} \sum_{i=1}^M L(\mathbf x^{(i)}, y^{(i)}, \boldsymbol \theta)
\end{align}  
where $L$ is the per-example loss depending on the input $\mathbf x$, the target $y$ and the model parameters $\boldsymbol \theta$. The minibatch size $M$ is typically chosen to be a relatively small number of examples, ranging from one to a few hundred \cite{Goodfellow2016}. Larger batches provide a more accurate estimate of the gradient, but with less than linear return. Multicore architectures are usually underutilized by extremely small batches. This motivatates using some absolute minimum batch size, below which there is no reduction in the time to precess a minibatch. Small batches can offer a regularization effect \cite{Wilson2003}, due to the noise they add to the learning process. Generalization error is often best for a batch size 1. Training with such a small batch size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can ba very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set. 

\paragraph{Back-Propagation}
When a feedforward neural network takes an input $\mathbf x$ and produce an output $\hat{\mathbf y}$, information flows forward through the network, which is called forward propagation. During training, forward propagation can continue onward until it produces a scalar loss $L(\boldsymbol \theta)$. The back-propagation algorithm \cite{Rumelhart1988} allows the information from the loss to then flow backward through the network in order to compute the gradient. Computing an analytical expression for the gradient is straightforward, but numerically evaluationg such an expression can be computationally expensive. The back-propagation algorithm does so using a simpe and inexpensive procedure. To obtain an algebraic expression for the gradient of a scalar it recursively applies the chain rule with respect to any node in the comultational graph that produced that scalar. 


\paragraph{Adam}

Adam \cite{Kingma2014} is another adaptive learning rate optimization algorithm. The name Adam derives from the phrase adaptive moments. It can be seen as a combination of RMSProp and momentum with a few important distinctions. First, in Adam, momentum is incorporated directly as an estimate of the first-order moment (with exponential weighting) of the gradient  

\paragraph{Batch normalization}


% check out cyclical learning rates \cite{Smith2017}



\subsection{Regularization}

Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization. Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. There is no best machine learning algorithm, and in particular, no best form of regularization \cite{Wolpert1996}. Instead the a form of regularization has to be chosen that is well suited to a particular task.



\paragraph{Weight Decay}


\paragraph{Early Stopping} 
Estimate generalization error with a validation set during training. Stop training when the error on the validation set rises (early stopping).

\paragraph{Dropout}



\subsection{Validation}

Usually the interest of a machine learning algorithm is in how well it performs on data that has not seen before, since this determines how well it will work when deployed in the real world. The ability to perform well on previously unobserved inputs is called generalization. The performance measure of the model is thus evaluated using a test set of data that is separated from the data used for training. What separates machine learning from optimization is that the generalization error, also called the test error, should be low as well. Formally, the generalization error is defined as the expected value of the error on a new input, where the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice.

There are two central challenges in machine learning, namely underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.

If the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the acailable data as possible for training. However, if the calidation set is small, it will give a relatively noisy estimate of predictive performance. One solution to this delemma is to use cross-validation.

\paragraph{Cross-validation}  The k-fold cross-validation alows a proportion $(k-1)/k$ of the availible date to be used for training while making use of all the data to assess performance. When data is particularly scarce, it may be appropiate to consider the case $k=N$, where $N $is the total number of data points, which gives the leave-one-out technique. One major drawback of cross-validation is that the number of training runs that must be performed is incresed by a factor of $k$, and this can prove problematic for models in which the training is itself computationally expensive.

\paragraph{Test set method} Split the observarions into to disjunct subsets 

Typically, one uses about 80 percent of thr training data for training and 20 percent for validation \cite{Goodfellow2016}. 

% \begin{align*}
% \text{observations}
% \begin{cases}
%     \text{ training data } \left\{ \left(\mathbf x^{(\alpha)}, \,\mathbf y_T^{(\alpha)}\right) \right \}, \, \alpha \in \{1,\dots, p\} & \\ 
%     \;\rightarrow E^T \text{ selects model parameters} & \\  \\
%     \text{ test data } \left\{ \left(\mathbf x^{(\beta)}, \,\mathbf y_T^{(\beta)}\right) \right \}, \, \beta \in \{1,\dots, q\} & \\    
%     \;\rightarrow \widehat{E}^G \text{ estimates generalization error} & \\ 
% \end{cases}
% \end{align*} 

% Old: Hyperparameter Optimization
\subsection{Hyperparameters}

Most machine learning algorithms have settings called hyperparameters, which control the behavior but are not adapted by the learning algorithm itself. Thus, hyperparameters must be determined outside the learning algorithm. One way to tune hyperparameters is a nested learning procedure in which one learning algorithm learns the best hyperparameters for another learning algorithm. 

Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because the setting is gradient-free and thus difficult to optimize.  


\begin{itemize}
\item optimzie hyperparameters with nevergrad \cite{nevergrad}.
\item instrumentation: turn a piece of code with parameters into a function defined on an n-dimensional continous data space
\end{itemize}


% \subsection{Multi Task Learning}

% \begin{itemize}
% \item It is the information in these extra training signals that helps the hidden layer learn a better internal representation for the door recognition domain, and this better representation in turn helps the net better learn to recognize [Caruana1997]
% \item Information in the extra tasks is helping the hidden layer learn a better internal representation
% \item 
% \end{itemize}


\newpage


\section{Deep Neural Networks}


Deep neural networks are called networks because they are typically represented by composing together many different functions, 
\begin{align}
f(\mathbf x) = (f^{(L)} \circ f^{(L-1)} \circ \dots \circ f^{(2)} \circ f^{(1)})(\mathbf x)
\end{align} 
where $f^{(1)}$ is called the first layer, $f^{(2)}$ is called the second layer, and so on. The overal length $L$ of the chain gives the depth of the model. The name deep learning arose from this terminology. The final layer is called output layer. The dimensionality of the hidden layers determines the width of the model. 

\subsection{Feedforward Neural Networks}

Feedforward neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models. The MLP was the first and simplest type of artificial neural network devised \cite{Schmidhuber2015}. In this network, the information moves in only one direction, forward, from the input nodes, to the output nodes. Thus, there are no cycles or loops in the network \cite{Zell1994}.

The goal of a feedforward network is to approximate some function $f^*$. A feedforward network defines a mapping $\mathbf y = f(\mathbf x; \boldsymbol \theta)$ and learns the value of the parameters $\boldsymbol \theta$ that result in the best funtion approximation. 

The model is associated with a directed acyclic graph describing how the functions are composed together.

Feedforward networks are of extreme importance to machine learning practicioners, because they form the bases of many important commercial applications. 


\subsection{Convolutional Neural Networks}

A convolutional neural network (CNN) \cite{LeCun1989} is a class of deep neural networks specialized for processing data that has a grid-like topology, such as one-dimensional time series or two-dimensional data like raster graphics. CNNs are regularized versions of feedforward neural networks that simply use convolution in place of general matrix multiplication in at least one of their layers \cite{Goodfellow2016}. Usually, the operation used in a convolutional neural network does not correspond precisely to the mathematical definition of convolution. Instead, many neural network libraries implement a cross-correlation, which is the same as convolution but flipping the second argument. In the following we follow this convention and define the convolution as
\begin{align}
s(t) = (x * w)(t) = \sum_{\tau=-\infty}^{\infty} x(a)\,w(t+\tau)
\end{align} 
where $x$ is referred to as the input, $w$ as the filter or kernel and the output $s$ is referred to as the feature map. Discrete convolution can be viewed as multiplication by a matrix, but the matrix has several entries constrained to be equalt to other entries. In addition, convolution usually corresponds to a very sparse matrix. This is because the filter is usually much smaller than the input. 

A typical layer of a convolutional neural network consists of three stages, as shown in Fig. \ref{fig:typical_cnn}. In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations. In the second stage wich is sometimes called detector stage, each linear activation is run through a nonlinear activation function. In the third stage, a pooling function is used to modify the output of the layer further. 

\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{figures/cnn.eps}
\caption{Typical convolutional neural network. }
\label{fig:typical_cnn}
\end{figure}

A pooling function replaces the output of the net a a certain location with a summary statistic of the nearby outputs. For example, the max pooling \cite{Zhou1988} operation reports the maximum output within a rectangular neighborhood. In this way, pooling helps to make the representation approximately invariant to small translations of the input. Invariance to translation means that when a input is translated by a small amount, the values of most of the pooled outputs do not change.

% In the context of machine learning, the learning algorithm will learn the appropriate values of the filter. 

% Convolutional neural networks (CNNs) \cite{LeCun1989} have been applied to sequences for decades. They were used prominently for speech recognition in the 80s and 90s.


\subsection{Recurrent Neural Networks}

In deep learning, a recurrent neural network (RNN) \cite{Rumelhart1988} is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. Unlike feedforward neural networks, RNNs contain cycles and use an internal state memory $\mathbf h$ to process sequences of inputs \cite{Elman1990}. A universal recurrent neural network is decribed by the propagation equations,
\begin{align}
\mathbf h_t &= \text{Activation}(\mathbf U \,\mathbf x_t + \mathbf W \, \mathbf h_{t-1} + \mathbf b) \\
\mathbf o_t &= \mathbf V \, \mathbf h_t + \mathbf c 
\end{align} 
where the parameters are the bias vectors $\mathbf b$ and $\mathbf c$ along with the weight matrices $\mathbf U$, $\mathbf V$ and $\mathbf W$, respectively, for input-to-hidden, hidden-to-output and hidden-to-hidden connections. The recurrent network maps an input sequence $\mathbf x_{1:T}$ of length $T$ to an output sequence $\mathbf o_{1:T}$ of the same length. The computational graph and its unfolded version is shown in Fig. \ref{fig:rnn}.
\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{figures/rnn.eps}
\caption{The computational graph on an recurrent neural network and its unfolded version.}
\label{fig:rnn}
\end{figure}    

Computing the gradinets involves performing a forward propagation pass through the unrolled graph followed by a backward propagation pass. The runtime is $O(T)$ and cannot be reduced by parallelization because the forward propagation graph is inherently sequential, i.e., each time step may be computed only after the previous one. Therefore, back-propagation for recurrent model is called back-propagartion through time (BPTT). Recurrent models construct very deep computational graphs by repeatedly applying the same operation at each time step of a long temporal sequence. This gives rise to the vanishing and exloding gradient problem and makes it notoriously difficult to train RNNs. To prevend these difficulties more elaborate recurrent architectures were developed, such as the LSTM \cite{Hochreiter1997} and the GRU \cite{Cho2014}. These families of architectures have gained tremendois popularity due to prominent applications to language modeling and machine translation. 

\paragraph{LSTM}
The long short-term memory (LSTM) \cite{Hochreiter1997} is an recurrent neural network (RNN) architecture used in the field of deep learning. It comprises leaky units to allow the network to accumulate information over a long duration. However, once that information has been used it might be useful for the network to forget the old state. Instead of manually deciding when to clear the state, the neural network learns to decide when to do it. The time scale of integration can be changed dynamically by making the weights gated, i.e., controllable by another hidden unit.

\paragraph{GRU}
The gated recurrent unit (GRU) \cite{Cho2014} is a gating mechanism in recurrent neural networks. The GRU is similar to an LSTM with forget gate but has fewer parameters than an LSTM, as it lacks an output gate. GRUs have been shown to exhibit even better performance on certain smaller datasets \cite{Chung2014}. However, the LSTM is strictly stronger than the GRU as it can easily perform unbounded counting, while the GRU cannot \cite{Weiss2018}.

% The intuitive appeal of recurrent modeling is that the hidden state can act as a representations of everything that has been seen so far in the sequence.

% Recurrent networks are dedicated sequence models that maintain a vector of hidden activations that are propagated throgh time \cite{Elman1990,Werbos1990,Graves2012}.






\subsection{Temporal Convolutional Networks}

A temporal convolutional network (TCN) \cite{Bai2018} represents a special kind of convolutional neural network and is informed by recent convolutional architectures for sequential data. It is designed from first principles and combines simplicity, autoregressive prediction, and very long memory. In comparison to WaveNet \cite{Oord2016}, the TCN does not employ skip connections across layers (no conditioning, context stacking, or gated activations).

The TCN is based upon two principles: 1) the convolutions are casual, i.e, no information leakage from future to past; 2) the architecture can take a sequence of any length and map it to an output sequence of the same length just as with an RNN. To achieve the first point, the TCN uses a 1D fully-convolutional network architecture \cite{Long2015}, where each hidden layer is the same length as the input layer. To accomplish the second point, the TCN uses causal convolutions, i.e., convolutions where an output at time $t$ is convolved only with elements from time $t$ and earlier in the previous layer.

Simple causal convolutions have the disadvantage to only look back at history with size linear in the depth of the network. To circumvent this fact, the architecture employs dilated convolutions that enable an exponentially large receptive field. More formally, for a 1-D sequence input $\mathbf x \in \mathbb R^T$ and a filter $f:\{ 0, \dots, k-1\} \rightarrow \mathbb R$, the delated convolution operation $F$ on element $s$ of the sequence is defined as
\begin{align}
F(s) = (\mathbf x *_d f)(s) = \sum_{i=0}^{k-1} f(i) \, \mathbf x_{s-d\cdot i}
\end{align}
where $d = 2^\nu$ is the dilation factor, with $\nu$ the level of the network, and $k$ is the filter size. The term $s-d\cdot i$ accounts for the direction of the past. Dilation is equivalent to introducing a fixed  step between every two adjacent filter taps, as it can be seen in Fig. \ref{fig:dilated_convolutions}. Using larger dilation enables an output at the top level to represent a wider range of inputs, thus effectivly expanding the receptive field of a CNN. There are two ways to increase the receptive field of a TCN: choosing lager filter sizes $k$ and increasing the dilation factor $d$, since the effective history of one layer is $(k-1) \, d$. 

\begin{figure}[htbp]
\centering
\includegraphics[scale=1]{figures/dilated_conv.eps}
\caption{A dilated casual convolution with dilation factors $d = 1,2,4$ and filter size $k=3$.}
\label{fig:dilated_convolutions} 
\end{figure}

Another architectual element of a TCN are residual connections. In place of a concolutional layer, TCNs employ a generic residual module. Each residual block contains a branch leading out to a series of transformations $\mathcal F$, whose outputs are added to the input $\mathbf x$ of the block, 
\begin{align}
o = \text{Activation} \big(\mathbf x + \mathcal F(\mathbf x)\big).
\end{align}
This effectively allows layers to learn modifications to the identity mapping rather than the entire transformation, which has been shown to benefit deep neural networks \cite{He2016}. Especially for very deep networks stabilization becomes important, for example, in the case where the prediction depends on a large history size ($> 2^{12}$) with a high-dimensional input sequence. 

A residual block has two layers of dilated causal convolutions and recfied linear units (ReLU) as non-linearity, shown in Fig. \ref{fig:residual_block}. For normalization, weight normalization \cite{Salimans2016} is applied to the convolutional filters. In addition, a spatial dropout \cite{Srivastava2014} is added after each dilated convolution for regularization, i.e., at each training step, a whole channel is zeroed out.
\begin{figure}[htbp]
    \centering
    \includegraphics[]{figures/residual_block.pdf}
    \caption{TCN residual block \cite{Bai2018}.}
    \label{fig:residual_block} 
\end{figure}


\begin{itemize}
\item the convolutions are casual, that means no information leakage from future to past (compare to bidirectional RNN).
\item long effective history sizes, i.e., the ability for networks to look very far into the past to make a prediction (receptive field?), by using a combination of very deep networks (augmented with residual layers) and dilated convolutions 
\item this allows layers to learn modifications to the identity mapping rather than the entire transformation, which has been shown to benefit deep neural networks (where?)
\item within a residual block there exist two layers of dilated causal convolution and a rectified linear unit (ReLU) \cite[Nair2010]{Nair2010} as a non-linearity
\item for normalization a weight normalization \cite[Salimans2016]{Salimans2016} is applied to the convolutional filters 
\item additionaly spatioal dropout \cite[Srivastava2014]{Srivastava2014} is added after each dilated convolution for regularization (at each training step, a whole channel is zeroed out)
\item The TCN architecture appers not inly more accurate than canonical recurrent networks such as LSTMs and GRUs, but also simpler and clearer.
\end{itemize}

\paragraph{LogSoftmax}

A LogSoftmax normalization is added the last layer of the neural network to obtain the log-probabilities for the differnent classes.
\begin{align}
\log \text{softmax}(x_i) = \log\left(\frac{\exp{(x_i)}}{\sum_j \exp(x_j)}\right)
\end{align} 


% \paragraph{Trellis Networks}
% \begin{figure}[htbp]
% \centering
% \includegraphics[scale=1.0]{figures/trellis_net_atomic_level.pdf}
% \caption{TrellisNet at an atomic level.}
% \label{fig:TrellisNetAtomic}
% \end{figure}    
% Input:  $x_{1:T} = x_1, \dots, x_T, \quad x_t \in \mathbb R^p, \quad x_{1:T} \in \mathbb R^{T \times p}$ \vspace{0.5em}\\ 
% with sequence length  $T$  input dimensionality  $p$
% Output: $y_{1:T} = y_1, \dots, y_T = G(x_1, \dots, x_T)$ \vspace{0.5em} \\
% with function $G: \mathcal X^T \rightarrow \mathcal Y^T$ 
% Hidden: $z_t^{(i)} \in \mathbb R^q$


\subsection{General-Purpose Computing on GPUs} 

In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU \cite{Oh2004}. In 2005, another paper also emphasised the value of GPGPU for machine learning.[42]


While GPUs operate at lower frequencies, they typically have many times the number of cores. Thus, GPUs can process far more pictures and graphical data per second than a traditional CPU. Migrating data into graphical form and then using the GPU to scan and analyze it can create a large speedup.


\newpage


% Old: Beat Tracking System
\section{Method}

From an overall perspective, the proposed automatic beat tracking system comprises three major stages, as it is shown in Fig. \ref{fig:system}. 

\begin{figure}[htbp]
\centering
\includegraphics[scale=1.0]{figures/beat_tracking_system.eps}
\caption{Beat tracking system signal flow}
\label{fig:system}
\end{figure}  


Seuqence-to-sequence prediction:
\begin{itemize}
\item the entire input sequence can be used to predict each output (not feasible in real time processing)
\end{itemize}

Binary classification problem:
\begin{itemize}
\item beat (class 1)
\item no beat (class 0)
\end{itemize}



\subsection{Dataset}
For training and evaluation we use the datasets listed in Table \ref{tab:datasets}.
\begin{table}[htbp]
\caption{Datasets used for training.}
\label{tab:datasets}
\centering
\begin{tabular}{lrr}
\hline
\hline
\textbf{Dataset} & \textbf{files} & \textbf{length} \\
\hline
Ballroom \cite{Gouyon2006b, Krebs2013} & $685$ & $5\,\text{h} \;57\,\text{m}$\\
GTZAN \cite{Tzanetakis2002b, marchand2015swing} & $1000$ & $8\,\text{h}\;20\,\text{m}$\\
Hainsworth \cite{Hainsworth2004} & $222$ & $3\,\text{h}\;19\,\text{m}$\\
SMC \cite{Holzapfel2012} & $217$ & $2\,\text{h}\;25\,\text{m}$\\    
\hline
Total & $ $ & $ \,\text{h}\; \,\text{m}$\\  
\hline
\hline
\end{tabular}
\end{table}  

\paragraph{Ballroom} 
The Ballroom dataset \cite{Gouyon2006b, Krebs2013} contains 685 audio files\footnote{After removing the 13 duplicates which are pointed out by Bob Sturm \cite{Sturm2014}.} of ballroom dancing music. As genres the files cover Cha Cha, Jive, Quickstep, Rumba, Samba, Tango, Viennese Waltz, and Slow Waltz. Each file is $30\,\text{s}$ long, mono and sampled at $44.1\,\text{kHz}$ with $16\text{-bit}$ resolution.

\paragraph{GTZAN} 
The GTZAN \cite{Tzanetakis2002b} dataset was originally proposed for music genre classification problem and later extended with beat annotations \cite{marchand2015swing}. The dataset comprises 1000 excerpts of 10 different genres. The genres are blues, classical, country, disco, hip-hop, jazz, metal, pop, reggae and rock. Each file is $30\,\text{s}$ long, mono and sampled at $22050\,\text{Hz}$ with $16\text{-bit}$ resolution. The audio content of the GTZAN dataset is representative of the real commercial music of various music genre. The dataset has a good balancing between tracks with swing (Blues and Jazz music) and without swing.

\paragraph{Hainsworth} The Hainsworth dataset \cite{Hainsworth2004} contains 222 musical audio files, with the following genre breakdown: Rock/Pop (68), Dance (40), Jazz (40); Classical (30), Folk (22), and Choral (22). Each file is between $30$ and $60\,\text{s}$ in length, mono and sampled at $44.1\,\text{kHz}$ with $16\text{-bit}$ resolution.

\paragraph{SMC}




\begin{itemize}
\item The ideal outcome of the annotation is an unambigous representation of the start points of musical events
\item Due to uncertainties in the annotation process, for many types of inpit signals (especially multi-instriment exerpts) it may not possible to determine onset locations with greater precision than $50\,\text{ms}$. \cite{Leveau2004}
\item The input data format is uncompressed linear pulse code modulated (PCM) signals. Single channel $16\,\text{bit}$ linear PCM with a sampling rate of $44100\,\text{Hz}$. The data is created by averaging the two channels of the original stereo recordings.
\end{itemize}


\subsection{Data Preprocessing}

\begin{itemize}
\item ``Is it important to use several features?'' [Durand2015]
\item Two different types of input features: Onset events and harmonic changes
\item Assumtion: Most harmonic changes occur inside a piece of music are located at metric bars \cite[Khadkevich2012]{Khadkevich2012}
\item in complex polyphonic mixtures of music, simultaneously occurring events of high intensities lead to masking effects that prevent any observation of an energy increase of a low intensity onset. To circumbent thesse masking effects, detection functions were proposed that analyze the signal in a bandwise fashion to extract transients occurring in certain frequency regions of the signal [Grosche2011]
\end{itemize}


% https://musicinformationretrieval.wordpress.com/2017/04/25/audio-beat-tracking-human-annotation-strategies/

The Beat Tracking task requires annotations in the form of time instants of beats from a musical excerpt. Additionally, the tempo, metrical level of the beat and downbeat positions can also be annotated for related tasks like downbeat tracking/ meter tracking/ tempo tracking etc.

The method for obtaining ground truth annotations depends on whether the aim is to identify descriptive beat locations or to replicate a human tapping response. In the former case, an initial estimate of the beat locations can be obtained by recording tap times for a given musical excerpt and iteratively modifying and auditing the obtained beat positions while in the latter case, the ground truth can be completely defined by the tapping response \cite{Davies2009b}.

The audio excerpt in time domain and its spectrogram can be visualised using tools like Sonic Visualizer. Beat locations can first be obtained by recording the tap locations the musical excerpt and then manually correcting these locations for exact onsets.  Annotators can also follow a semi-automatic process, for example in [2], where beats and downbeats were first obtained through the ircambeat software and then errors were manually corrected by human annotators.

After preprocessing the audio we obtain the observation set $ O = \left\{ \mathbf x^{(\alpha)}, \,\mathbf y_T^{(\alpha)} \right \}_{\alpha = 1}^p$, with $p$ samples in total. 

\begin{itemize}
\item Data augmentation (transformation or adding noise) $\Rightarrow$ prediction gets robust against transformation and noisy signals
\item output: $y$ probability of beat instant per time frame 
\end{itemize}



The first step in the approach consists of preprocessing the original data. Data preprocessing refers to all transformations on the raw data before the resulting training set is fed to the machine learning algorithm. It includes different methods such as normalization, transformation and feature extraction. 

The data set contains raw pulse code modulated (PCM) audio signals stored as WAV files. Loose redundancy of stereo file. For the sake of consistency and also to reduce computational complexity the audio signal is resampled at a sampling rate $f_s = 44.1 \,\text{kHz}$ and converted to a monaural signal by averaging both stereo channels. 


\begin{itemize}
\item Slice audio $x(t)$ into frames $x_n(t)$, $n = 1, 2,\dots, N$, where $N$ is the number of frames
\item Compute complex spectrogram $X(n,k)$ with FFT 
\item Complex spectrogram is converted to the power spectrogram $S(n, k) = |X(n, k)|^2$
\item Mel-spectrogram $M(n,m) = \log \left( S(n,k) \cdot F(m,k)^T + 1.0 \right)$
\end{itemize}
\vspace{1em}

Short-time Fourier transform:
\begin{align}
X(n,k) = \sum_{m = 0}^{N-1} w(m) \, x(m + n  h) \, e^{-2 \pi j  k /N}
\end{align} 

Spectrogram: 
\begin{align}
S(n,k) = |X(n,k)|^2
\end{align} 

Separation of magnitude and phase:
\begin{align} 
X(n,k) = |X(n,k)| \, e^{\, j \phi(n,k)}
\end{align} 


\paragraph{Chroma Representation}

\begin{itemize}
\item Chords are more likely to change in beat times than on other positions. Chords are more likely to change at the beginnings of measures than at other positions of the beat.
\item A chromagram comprises a time-series of chroma vectors. which represent harmonic content at a specific time in the audio.
\item Chromagrams are concise descriptors of harmony because they encode tone quality and neglect tone height.
\item CRP (chroma DCT-reduced log-pitch) features have significant amount of robustness to changes in timbre an instrumentation \cite[Mueller2010]{Mueller2010}
\item Employ data-driven approach to extract chromagrams that specifically encode content relevant to harmony 
\item Chroma features are noisy in their basic formulation be- cause they are affected by various interferences: musical instruments produce overtones in addition to the funda- mental frequency; percussive instruments pollute the spec- trogram with broadband frequency activations (e.g. snare drums) and/or pitch-like sounds (tom-toms, bass drums); different combinations of instruments (and different, pos- sibly genre-dependent mixing techniques) create different timbres and thus increase variance [7,20] \cite{Korzeniowski2016}
\end{itemize}


\subsection{Feature Learning}



\subsection{Temporal Decoding}
\label{sec:temporal_decoding}

The metrical structure of music builds upon a hierarchy of appriximatey regular pulses, as elaborated in Section \ref{sec:perception}. To explot this seqential structure, a probabilistic dynamic model is used to result in a more robust estimation of the beat instants and to correctly operate under rhythmic fluctuations, such as ritardando and accelarando or metric modulations.

As a postprocessing stage following the feature learning, a dynamic Bayesian network (DBN) is employed which jointly infers tempo and the phase of the beat. Dynamic Bayesian networks are popular frameworks for meter tracking in music becuase they are able to incorporate prior knowledge about the dynamics of rhythmic parameters. 
In this DBN, which is a hidden Markov model (HMM) precisely, a sequence of hidden variables that represent the meter of an audio piece is inferred from a sequence of observed variables. The probabilistic state space consist of two hidden variables, the position within a bar and the tempo. 

The probabilistic model is based upon the bar pointer model of Whiteley et al. \cite{Whiteley2006} which enables joint inference of rhythmic parameters from a piece of music. Mutual dependencies between these parameters are exploited, which increases the computational complexity of the models. A bar pointer is defined as being a hypothetical hidden object located in state space consiting of the period of a latent rhythmical pattern. The velocitiy of the bar pointer is defined to be proportional to tempo. The advantage of this approach is that the bar pointer continues to move whether or not a beat activation is observed. This explicitly models the concept that meter is a latent process and provides robustness against rest in the music which might otherwise be wrongly interpreted as local variations in tempo. In order to make inference computationally tractable, the state-space is usually divided into discrete cells \cite{Krebs2015}. 

Given a sequence of observed data $a_{1:T}$, we wish to identify the most probable hidden state trajectory $\mathbf s_{1:T}$. At each time frame $t$, the bar pointer of the hidden state space is referred to as $\mathbf s_t = [\phi_t, \nu_t]$, with $\phi_t \in \{1, 2, \dots, K\}$ denoting the position within a bar, and $\nu_t \in \{ K_{\text{min}}, K_{\text{min}}+1, \dots, K_{\text{max}}\}$ referring to as the tempo. The number of discrete bar positions $K$ is dependent on the tempo by using exactly one bar position state per audio frame and thus per observation feature value. The number of observations per bar (four beats) at a tempo $\Theta$ in beats per minute (BPM) is
\begin{align}
K(\Theta) = \left\lfloor \frac{4 \times 60}{\Theta \, \Delta} \right\rfloor 
\label{eq:number_of_obstervations_per_bar}
\end{align} 
where $\Delta$ is reffered to as the audio frame length. With Equation. \ref{eq:number_of_obstervations_per_bar}, one can compute the number of bar positions of the tempo limits $K_\text{min} = K(\Theta_\text{max})$ and $K_\text{max} =K(\Theta_\text{min})$. Using a tempo range of  $\Theta = [55, 215]$ the postion whithin a bar has at most $82$ tempo states.

With this state space discretisation, the most likely hidden state sequence $\mathbf s_{1:T}^* = \mathbf s_1^*, \dots, \mathbf s_T^*$ given a sequence of observations $a_{1:T} = a_1, \dots, a_T$ is computed by
\begin{align}
\mathbf s_{1:T}^* = \arg \max_{\mathbf s_{1:T}} P(\mathbf s_{1:T}\,|\, a_{1:T})
\label{eq:most_likely_states}
\end{align} 
with 
\begin{align}
 P(\mathbf s_{1:T}\,|\, a_{1:T}) \propto P(\mathbf s_1) \prod_{t=2}^T P(\mathbf s_t\,|\,\mathbf s_{t-1})\, P( a_t\,|\,\mathbf s_{t})
\end{align} 
where $P(\mathbf s_1)$ is the initial state distribution, $P(\mathbf s_t\,|\,\mathbf s_{t-1})$ is the transition model, and $P(\mathbf a_t\,|\,\mathbf s_{t})$ is the observation model. The most likely hidden state sequence can be solved using the well-known Viterbi algorithm \cite{Viterbi1967}. Finally, the set of beat instant $\mathcal B$ can be extracted from the sequence of bar positions as
\begin{align}
\mathcal B = \{ t: \phi_t^*=1\}
\end{align} 
In order to infer the hidden variables from an audio signal, the three entities are specified. The rransition model describes the transitions between the hidden variables. The observation model takes the beat activations from the neural network
and initial distribution encodes prior knowledge about the hidden variables.

\paragraph{Initial Distribution} 
Any prior knowledge about tempo distributions can be incorporated into the model. For example, if the music to be tracked from one genre, a specific tempo distribution can be used. To make it genre independend, a uniform distribution is used in this theses. 


\paragraph{Transition Model} The transition model $P(\mathbf s_t\,|\,\mathbf s_{t-1})$ can be further decomposed into a distribution for each of the two hidden variables $\phi_t$ and $\nu_t$, this is
\begin{align}
P(\mathbf s_t\,|\,\mathbf s_{t-1}) = P(\mathbf \phi_t\,|\,\mathbf \phi_{t-1}, \nu_{t-1}) \, P(\nu_t \, |\, \nu_{t-1}) 
\end{align} 
where the first factor is
\begin{align}
P(\mathbf \phi_t\,|\,\mathbf \phi_{t-1}, \nu_{t-1}) = \mathbf 1_A
\end{align} 
with the indicator fuction $\mathbf 1_A$ that equals one if $\phi_t = (\phi_{t-1}+\nu_{t-1}-1)\mod M+1$. The modulo operator makes the bar postion cyclic. If $\phi_t \in \mathcal B$, the second factor is defined by
\begin{align}
P(\nu_t \, |\, \nu_{t-1}) = \exp \left( -\lambda \left| \frac{\nu_{t}}{\nu_{t-1}} -1 \right| \right)
\end{align} 
otherwise it is
\begin{align}
P(\nu_t \, |\, \nu_{t-1}) = \begin{cases}
    1, &\nu_t = \nu_{t-1}\\
    0, &\text{else.} 
\end{cases}
\end{align} 
The parameter $\lambda \in \mathbb Z_{\geq0}$ determines the steepness of the distribution and models the probability for a tempo change. A value of $\lambda = 0$ means that transitions to all tempi are equally probable. In practice, for music with roughly constant tempo, we set $\lambda \in [1, 300]$. The probability of tempo change is heuristically set to $p_\omega = 0.002$. Higher-level or domain specific knowledge could be used to set this parameter. For example in rock or pop music, the beat is usually quite steady, so a samll value for $p_\omega$ would be quite appropriatre, while for classical music, particularly styles including many tempo changes, a higher value would be more optimal.


\paragraph{Observation Model}
The beat activation function produced by the neural network is limited to the range $[0, 1]$ and shows high values at beat positions and low values at non-beat positions. Thus, the activation function is used directly as state-conditional observation distributions \cite{Degara2012}. The observation likelihood is defined as 
\begin{align}
P(a_t\,|\,\phi_t) = \begin{cases}
    a_t, &1 \leq \phi_t\leq \frac{\lambda}{\Lambda}\\
    \frac{1-a_t}{\lambda-1}, &\text{else}.    
\end{cases}
\end{align} 


 % \item The tempo in beat positions per time frame $\dot \Phi \in \{M(T_{\text{max}}), M(T_{\text{max}}) + 1,  \dots, M(T_{\text{min}})\}$ 
% \item For example: 
% \begin{align*}
% T_{\text{min}} = 56 \,\text{BPM} & \;\rightarrow \; M(T_{\text{min}}) = 107 \\
% T_{\text{max}} = 215 \,\text{BPM} &\;\rightarrow \; M(T_{\text{max}}) = 28 
% \end{align*} 
% \vspace{-1.5em}
% \begin{align*}
% &\Phi \in \{1, 2, \dots ,107\} \\
% &\dot \Phi \in \{28, 29, \dots ,107\}
% \end{align*} 


% \subsection{Time-Frequency Reassignment}
% \begin{itemize}
% \item audio signals have a distribution of energy that varies in time and frequency
% \item a spectrogram is constrained by an unfortunate tradeoff between resolution in time and frequency
% \item time-frequency reassignment: mapping the data to time-frequency coordinates that are nearer to the true region of support of the analyzed signal
% \item time-frequency reassignment has been used in a variety of applications for obtaining improved time and frequency estimates for time-varying spectral data [cite... 16-18]
% \end{itemize}

% \paragraph{Potential features:} Zero Crossing Rate, Energy, Danceability, Entropy of Energy, Spectral Centroid, Spectral Spread, Spectral Entropy, Spectral Flux, Spectral Roll off, MFCC, Chroma Vector, Chroma Deviation


% \section{Clustering}
% The whole training data gets separated in different clusters. Each cluster represents a homogenous subset of the whole data set, i.e., the multiple models are trained for specific aspects (e.g. genre, timbre, rhythmical patterns, instrumentation, etc.). 

% basic principles of how humans understand rhythms









\newpage
\section{Evaluation}

% Discontinuity in the pulse see Gouyon2005 page 3

\begin{itemize}
\item annotations tapped at different metrical levels in beat tracking
\item a evaluation method should adequately contend with the inherent uncertainty and/or ambiguity while providing a measurement of performance which is both meaningful and easy to interpret. \cite{Davies2009b}
\item objective methods for beat tracking evaluation compare the output beat times from a beat tracking algorithm against one or more sequences of ground truth annotated beat times. \cite{Davies2009b}
\item We must question the imprtance of strict continuity when evaluating a beat tracking system. A single misplaced beat, in the context of a sequence of otherwise accurate beats, is a far less disturbing error than the accuracy calculated with the ontinuity requiremet would suggest. 
\end{itemize}


\subsection{Performance Measure}

Set of annotated beat times $\mathcal A = \{a_1, \dots, a_A\}$ \\
Set of predicted beat times $\mathcal B = \{b_1, \dots, b_B\}$ \\

True positive if beat is in $\pm 70\,\text{ms}$ rage of annotated beat

$tp =$ number of true positives \\
$fp =$ number of false positives (extra detections) \\
$fn =$ number of false negatives (missed detections) 

\paragraph{Precision and recall:} $ $

$\text{precision} = \dfrac{tp}{tp + fp}\;, \qquad \text{recall} = \dfrac{tp}{tp + fn}$


\paragraph{F-measure:} $ $

$F_1 = 2 \cdot \dfrac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}} = \dfrac{2\, tp}{2\, tp + fp + fn}$

\paragraph{P-score:} Define two sequences $T_a$ and $T_b$ as 

$T_a(n) = \begin{cases}
	1, &\text{ if } n \in \mathcal A \\
	0 , & \text{ otherwise }
\end{cases}, \qquad T_b(n) = \begin{cases}
	1, &\text{ if } n \in \mathcal B \\
	0 , & \text{ otherwise }
\end{cases}$

$\text{P-score} = \dfrac{\displaystyle \sum_{m=-w}^w  \sum_{n}\,  T_a(n)\,  T_b(n+m)}{\max(A,B)} \;, \qquad \text{where } w = 0.2 \, \text{median}(\Delta_a)$ 



\subsection{Saliency Maps}

\begin{itemize}
\item How do the filters of the ConvNet look like?
\item Artificial neural networks often give good results, but it is difficult to understand what they learned, or on which basis they generate their output.
\item compute saliency maps using guided back-propagation [25]
\end{itemize}



\subsection{Network Training}

\begin{itemize}
\item To achieve better results, we could use DNN ensembles instead of a single DNN. We could ensure that the network sees data for which its pre- dictions are wrong more often during training, or similarly, we could simulate a more balanced dataset by showing the net super-frames of rare chords more often \cite{Korzeniowski2016}
\end{itemize}



\subsection{Labeling}

\begin{itemize}
\item In an editor (Sonic Visualizer) that enables a user to mark beat positions in a digitalized audio signal while listening to the audio and watching its waveform.
\item The positions can be finely adjusted by playing back the audio with click tones at beat times.
\end{itemize}


\newpage 

\section{Conclusion}

\begin{itemize}
\item Theoretically, a single network large enough should be able to model all differnt music styles simultaneously but unfortunately this optional solution is hardly achievable. The main reason for this is the diffuculty to choose an absolutely balanced training set with an evenly distributed set of beats over all the different dimensions relevant for detecting beats. These include rhythmic patterns, harmonic aspects and many other features
\item We have presented an efficient audio based beat tracking algorithm able to produce equivalante results to the current state-of-the-art. 
\item Limitations in both the amount of training data and the variable quality of the annotations. Perform data augmentation. 
\item Dataset size: ``\emph{[...] a rough rule of thumb is that a supervised deep learning algorithm will gernerally achieve acceptable performance with around 5000 labeled examples per category and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples.}'' \cite[Goodfellow2016]{Goodfellow2016}
\item The most mentionable aspect is that the neural networks were trained solely on ballroom dance and other kinds of western pop music. The networks accuracy depends on the trainings set. Selective bias. 
\item Leave higher level analysis to the neural network
\item System operates causaly, thus an real-time implementation is possible.
\end{itemize}

\newpage

\section*{General notes}
\begin{itemize}
\item Goal of supervised learning: predict not yet observed input
\item Motivation: ``\emph{It is probable that no universal beat tracking model exist which does not utilise a switching model to recognize style and context prior to application.}'' \cite[Collins2006]{Collins2006}
\item Instead of calculating the STFT as for input featuress discover a good set of features by representation learning. (``\emph{Learned representations often result in much better performance than can be obtained with hand-designed representations.}'' \cite[Goodfellow2016]{Goodfellow2016})
\item Common association between sequence modeling and recurrent networks: ``\emph{Given a new sequence modeling task or dataset, wich architecture should one use?}'' \cite[Bai2018]{Bai2018}
\item Certain convolutional architectures can reach state-of-the-art accuracy in audio synthesis (e.g. Google WaveNet \cite[Oord2016]{Oord2016})
\item Do TCNs outperform LSTM architectures in this particular sequence modeling task?
\item Contrast of the two statements:
\begin{itemize}
\item ``\emph{Feature extraction is an essential step towards efective and accurate beat/downbeat positions extraction.}'' \cite[Khadkevich2012]{Khadkevich2012}
\item ``\emph{The system shows state-of-the-art beat and downbeat tracking performance on a wide range of different musical genres and sryles. It does so ny avoiding hand-crafted features such as harmonic changes, or rhythmic patterns, but rather leans the relevant features directly from audio.}'' \cite[Boeck2016b]{Boeck2016b}
\end{itemize} 
\item Hainsworth and Macleaod state that beat tracking systems will have to be style specific in the future in order to improve the state-of-the-art \cite{Hainsworth2004}.
\item The no free lunch therorem fo machine learning \cite{Wolpert1996} states that, averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserverd points. This implies that we must design our machine learning algorithms to perform well on a specific task.
\end{itemize}

\section*{Appendix}

\paragraph{Hyperparameter} 
\begin{itemize}
\item window function (e.g. Hamming window)
\end{itemize}

\newpage 
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}