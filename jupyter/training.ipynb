{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from python.utils import *\n",
    "from python.optimizations import (weight_norm, VariationalDropout, \n",
    "    VariationalHidDropout, WeightDrop, embedded_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.0.0\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Torch version: {}'.format(torch.__version__))\n",
    "print('Device: %s' % (device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 'Dataset' and 'DataLoader' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data.index = np.arange(len(self.data))\n",
    " \n",
    "    def __getitem__(self, i): \n",
    "        return get_input(self.data.at[i,'file']), get_labels(self.data.at[i,'file'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "     \n",
    "# merges a list of samples to form a mini-batch\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "    \n",
    "    features = pad_sequence(features, batch_first=True)\n",
    "    \n",
    "    labels = pad_sequence(labels, batch_first=True)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data and split it into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just ballroom \n",
    "ballroom = data[data['data_set'] == 1]\n",
    "\n",
    "np.random.seed(1111)\n",
    "\n",
    "train_valid_ratio = 0.9\n",
    "\n",
    "train_indices = np.sort(np.random.choice(ballroom['idx'], int(len(ballroom)*train_valid_ratio), replace=False))\n",
    "valid_indices = np.sort(np.array(list(set(np.arange(len(ballroom))) - set(train_indices))))\n",
    "\n",
    "train_set = Data(ballroom.iloc[train_indices])\n",
    "valid_set = Data(ballroom.iloc[valid_indices])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, \n",
    "                         drop_last=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(dataset=valid_set, batch_size=len(valid_set), collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "class ModelBoeck(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelBoeck, self).__init__()\n",
    "        \n",
    "        # Model parameters\n",
    "        self.input_size = 120\n",
    "        self.output_size = 2\n",
    "        self.num_layers = 3\n",
    "        self.hidden_size = 25     \n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        \n",
    "        # Recurrent layer \n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, \n",
    "                            hidden_size=self.hidden_size, \n",
    "                            num_layers=self.num_layers, \n",
    "                            bidirectional=self.bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # Read out layer\n",
    "        self.fc = nn.Linear(self.num_directions * self.hidden_size, self.output_size)       \n",
    "        \n",
    "    def forward(self, x):         \n",
    "        lstm_out, _ = self.lstm(x)         \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        y = F.log_softmax(fc_out, dim=2)\n",
    "        return torch.transpose(y, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBoeck().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# TCN\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    \n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        output = self.linear(output).double()        \n",
    "        output = F.log_softmax(output, dim=2).transpose(1, 2)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of learnable Parameters: 624702\n"
     ]
    }
   ],
   "source": [
    "input_size = 120\n",
    "output_size = 2\n",
    "nhid = 100\n",
    "levels = 6\n",
    "n_channels = [nhid] * levels\n",
    "kernel_size = 5\n",
    "dropout = 0.25\n",
    "\n",
    "model = TCN(input_size, output_size, n_channels, kernel_size, dropout=dropout).to(device)\n",
    "\n",
    "print('Number of learnable Parameters: {}'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trellis Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0,
     162
    ]
   },
   "outputs": [],
   "source": [
    "# Trellis Net\n",
    "\n",
    "class WeightShareConv1d(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_out, kernel_size, dropouth=0.0):\n",
    "        \"\"\"\n",
    "        The weight-tied 1D convolution used in TrellisNet.\n",
    "\n",
    "        :param input_dim: The dim of original input\n",
    "        :param hidden_dim: The dim of hidden input\n",
    "        :param n_out: The dim of the pre-activation (i.e. convolutional) output\n",
    "        :param kernel_size: The size of the convolutional kernel\n",
    "        :param dropouth: Hidden-to-hidden dropout\n",
    "        \"\"\"\n",
    "        super(WeightShareConv1d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_inp1 = input_dim\n",
    "\n",
    "        conv1 = nn.Conv1d(input_dim, n_out, kernel_size)\n",
    "        self.weight1 = conv1.weight\n",
    "\n",
    "        conv2 = nn.Conv1d(hidden_dim, n_out, kernel_size)\n",
    "        self.weight2 = conv2.weight\n",
    "        self.bias2 = conv2.bias\n",
    "        self.init_weights()\n",
    "\n",
    "        self.dict = dict()\n",
    "        self.drop = VariationalHidDropout(dropout=dropouth)\n",
    "\n",
    "    def init_weights(self):\n",
    "        bound = 0.01\n",
    "        self.weight1.data.normal_(0, bound)\n",
    "        self.weight2.data.normal_(0, bound)\n",
    "        self.bias2.data.normal_(0, bound)\n",
    "\n",
    "    def forward(self, input, dilation, hid=None):\n",
    "        k = self.kernel_size\n",
    "        padding = (k - 1) * dilation    # To maintain causality constraint\n",
    "        x = F.pad(input, (padding, 0))\n",
    "\n",
    "        # Input part\n",
    "        x_1 = x[:, :self.n_inp1]\n",
    "\n",
    "        # Hidden part\n",
    "        z_1 = x[:, self.n_inp1:]\n",
    "        z_1[:, :, :padding] = hid.repeat(1, 1, padding)  # Note: we only pad the hidden part :-)\n",
    "        device = x_1.device\n",
    "        \n",
    "        # A linear transformation of the input sequence (and pre-computed once)\n",
    "        if (dilation, device) not in self.dict or self.dict[(dilation, device)] is None:\n",
    "            self.dict[(dilation, device)] = F.conv1d(x_1, self.weight1, dilation=dilation)\n",
    "\n",
    "        # Input injection\n",
    "        return self.dict[(dilation, device)] + F.conv1d(self.drop(z_1), self.weight2, self.bias2, \n",
    "                                                        dilation=dilation)\n",
    "\n",
    "\n",
    "class TrellisNet(nn.Module):\n",
    "    def __init__(self, ninp, nhid, nout, nlevels=40, kernel_size=2, dropouth=0.0,\n",
    "                 wnorm=True, aux_frequency=20, dilation=[1]):\n",
    "        \"\"\"\n",
    "        Build a trellis network with LSTM-style gated activations\n",
    "\n",
    "        :param ninp: The input (e.g., embedding) dimension\n",
    "        :param nhid: The hidden unit dimension (excluding the output dimension). In other words, \n",
    "                     if you want to build a TrellisNet with hidden size 1000 and output size 400, \n",
    "                     you should set nhid = 1000-400 = 600.\n",
    "                     (The reason we want to separate this is from Theorem 1.)\n",
    "        :param nout: The output dimension\n",
    "        :param nlevels: Number of layers\n",
    "        :param kernel_size: Kernel size of the TrellisNet\n",
    "        :param dropouth: Hidden-to-hidden (VD-based) dropout rate\n",
    "        :param wnorm: A boolean indicating whether to use weight normalization\n",
    "        :param aux_frequency: Frequency of taking the auxiliary loss; (-1 means no auxiliary loss)\n",
    "        :param dilation: The dilation of the convolution operation in TrellisNet\n",
    "        \"\"\"\n",
    "        super(TrellisNet, self).__init__()\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.h_size = h_size = nhid + nout\n",
    "        self.dilation = dilation\n",
    "        self.nlevels = nlevels\n",
    "        self.fn = None\n",
    "        self.last_output = None\n",
    "        self.aux_frequency = aux_frequency\n",
    "\n",
    "        self.kernel_size = ker = kernel_size\n",
    "\n",
    "        if wnorm:\n",
    "            print(\"Weight normalization applied\")\n",
    "            self.full_conv, self.fn = weight_norm(\n",
    "                WeightShareConv1d(ninp, h_size, 4 * h_size, kernel_size=kernel_size, dropouth=dropouth),\n",
    "                names=['weight1', 'weight2'],\n",
    "                dim=0)           # The weights to be normalized\n",
    "        else:\n",
    "            self.full_conv = WeightShareConv1d(ninp, h_size, 4 * h_size, kernel_size=ker, dropouth=dropouth)\n",
    "\n",
    "    def transform_input(self, X):\n",
    "        # X has dimension (N, ninp, L)\n",
    "        batch_size = X.size(0)\n",
    "        seq_len = X.size(2)\n",
    "        h_size = self.h_size\n",
    "\n",
    "        self.ht = torch.zeros(batch_size, h_size, seq_len).to(device)\n",
    "        self.ct = torch.zeros(batch_size, h_size, seq_len).to(device)\n",
    "        return torch.cat([X] + [self.ht], dim=1)     # \"Injecting\" input sequence at layer 1\n",
    "\n",
    "    def step(self, Z, dilation=1, hc=None):\n",
    "        ninp = self.ninp\n",
    "        h_size = self.h_size\n",
    "        (hid, cell) = hc\n",
    "\n",
    "        # Apply convolution\n",
    "        out = self.full_conv(Z, dilation=dilation, hid=hid)\n",
    "\n",
    "        # Gated activations among channel groups\n",
    "        ct_1 = F.pad(self.ct, (dilation, 0))[:, :, :-dilation]  # Dimension (N, h_size, L)\n",
    "        ct_1[:, :, :dilation] = cell.repeat(1, 1, dilation)\n",
    "\n",
    "        it = torch.sigmoid(out[:, :h_size])\n",
    "        ot = torch.sigmoid(out[:, h_size: 2 * h_size])\n",
    "        gt = torch.tanh(out[:, 2 * h_size: 3 * h_size])\n",
    "        ft = torch.sigmoid(out[:, 3 * h_size: 4 * h_size])\n",
    "        ct = ft * ct_1 + it * gt\n",
    "        ht = ot * torch.tanh(ct)\n",
    "\n",
    "        # Put everything back to form Z (i.e., injecting input to hidden unit)\n",
    "        Z = torch.cat([Z[:, :ninp], ht], dim=1)\n",
    "        self.ct = ct\n",
    "        return Z\n",
    "\n",
    "    def forward(self, X, hc, aux=True):\n",
    "        ninp = self.ninp\n",
    "        nout = self.nout\n",
    "        Z = self.transform_input(X)\n",
    "        aux_outs = []\n",
    "        dilation_cycle = self.dilation\n",
    "\n",
    "        if self.fn is not None:\n",
    "            # Recompute weight normalization weights\n",
    "            self.fn.reset(self.full_conv)\n",
    "        for key in self.full_conv.dict:\n",
    "            # Clear the pre-computed computations\n",
    "            if key[1] == X.device:\n",
    "                self.full_conv.dict[key] = None\n",
    "        self.full_conv.drop.reset_mask(Z[:, ninp:])\n",
    "\n",
    "        # Feed-forward layers\n",
    "        for i in range(0, self.nlevels):\n",
    "            d = dilation_cycle[i % len(dilation_cycle)]\n",
    "            Z = self.step(Z, dilation=d, hc=hc)\n",
    "            if aux and i % self.aux_frequency == (self.aux_frequency-1):\n",
    "                aux_outs.append(Z[:, -nout:].unsqueeze(0))\n",
    "\n",
    "        out = Z[:, -nout:].transpose(1, 2)              # Dimension (N, L, nout)\n",
    "        hc = (Z[:, ninp:, -1:], self.ct[:, :, -1:])     # Dimension (N, h_size, L)\n",
    "        if aux:\n",
    "            aux_outs = torch.cat(aux_outs, dim=0).transpose(0, 1).transpose(2, 3)\n",
    "        else:\n",
    "            aux_outs = None\n",
    "        return out, hc, aux_outs\n",
    "    \n",
    "class MixSoftmax(nn.Module):\n",
    "    def __init__(self, n_components, n_classes, nlasthid, ninp, decoder, dropoutl):\n",
    "        \"\"\"\n",
    "        Apply mixture of softmax on the last layer of the network\n",
    "\n",
    "        :param n_components: The number of softmaxes to use\n",
    "        :param n_classes: The number of classes to predict\n",
    "        :param nlasthid: The dimension of the last hidden layer from the deep network\n",
    "        :param ninp: The embedding size\n",
    "        :param decoder: The decoder layer\n",
    "        :param dropoutl: The dropout to be applied on the pre-softmax output\n",
    "        \"\"\"\n",
    "        super(MixSoftmax, self).__init__()\n",
    "        self.n_components = n_components\n",
    "        self.n_classes = n_classes\n",
    "        self.prior = nn.Linear(nlasthid, n_components)  # C ---> m\n",
    "        self.latent = nn.Linear(nlasthid, n_components * ninp)  # C ---> m*C\n",
    "        self.decoder = decoder\n",
    "        self.var_drop = VariationalDropout()\n",
    "        self.ninp = ninp\n",
    "        self.nlasthid = nlasthid\n",
    "        self.dropoutl = dropoutl\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.prior.weight.data.uniform_(-initrange, initrange)\n",
    "        self.latent.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, context):\n",
    "        n_components = self.n_components\n",
    "        n_classes = self.n_classes\n",
    "        decoder = self.decoder\n",
    "        ninp = self.ninp\n",
    "        dim = len(context.size())\n",
    "\n",
    "        if dim == 4:\n",
    "            # context: (N, M, L, C)  (used for the auxiliary outputs)\n",
    "            batch_size = context.size(0)\n",
    "            aux_size = context.size(1)\n",
    "            seq_len = context.size(2)\n",
    "            priors = F.softmax(self.prior(context), dim=3).view(-1, n_components)  # (M*N*L, m)\n",
    "            latent = self.var_drop(self.latent(context), self.dropoutl, dim=4)\n",
    "            latent = F.softmax(decoder(F.tanh(latent.view(-1, n_components, ninp))), dim=2)\n",
    "            return (priors.unsqueeze(2).expand_as(latent) * latent).sum(1).view(batch_size, aux_size, seq_len,\n",
    "                                                                                n_classes)\n",
    "        else:\n",
    "            batch_size = context.size(0)\n",
    "            seq_len = context.size(1)\n",
    "            priors = F.softmax(self.prior(context), dim=2).view(-1, n_components)  # (N*L, m)\n",
    "            latent = self.var_drop(self.latent(context), self.dropoutl)\n",
    "            latent = F.softmax(decoder(F.tanh(latent.view(-1, n_components, ninp))), dim=2)  # (N*L, m, n_classes)\n",
    "            return (priors.unsqueeze(2).expand_as(latent) * latent).sum(1).view(batch_size, seq_len, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TrellisNetModel(nn.Module):\n",
    "    def __init__(self, ninp, nhid, nout, nlevels, kernel_size=2, dilation=[1], dropout=0.0, dropouti=0.0, \n",
    "                 dropouth=0.0, dropoutl=0.0, emb_dropout=0.0, wdrop=0.0, temporalwdrop=True, \n",
    "                 repack=False, wnorm=True, aux=True, aux_frequency=20, n_experts=0):\n",
    "        \"\"\"\n",
    "        A deep sequence model based on TrellisNet\n",
    "\n",
    "        :param ninp: The input dimension\n",
    "        :param nhid: The hidden unit dimension (excluding the output dimension). In other words, \n",
    "                     if you want to build a TrellisNet with hidden size 1000 and output size 400, \n",
    "                     you should set nhid = 1000-400 = 600.\n",
    "                     (The reason we want to separate this is from Theorem 1.)\n",
    "        :param nout: The output dimension\n",
    "        :param nlevels: The number of TrellisNet layers\n",
    "        :param kernel_size: Kernel size of the TrellisNet\n",
    "        :param dilation: Dilation size of the TrellisNet\n",
    "        :param dropout: Output (variational) dropout\n",
    "        :param dropouti: Input (variational) dropout\n",
    "        :param dropouth: Hidden-to-hidden (VD-based) dropout\n",
    "        :param dropoutl: Mixture-of-Softmax dropout (only valid if MoS is used)\n",
    "        :param emb_dropout: Embedding dropout\n",
    "        :param wdrop: Weight dropout\n",
    "        :param temporalwdrop: Whether we drop only the temporal parts of the weight (only valid if wdrop > 0)\n",
    "        :param repack: Whether to use history repackaging for TrellisNet\n",
    "        :param wnorm: Whether to apply weight normalization\n",
    "        :param aux: Whether to use auxiliary loss (deep supervision)\n",
    "        :param aux_frequency: The frequency of the auxiliary loss (only valid if aux == True)\n",
    "        :param n_experts: The number of softmax \"experts\" (i.e., whether MoS is used)\n",
    "        \"\"\"        \n",
    "        super(TrellisNetModel, self).__init__()\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.dropout = dropout    # Rate for dropping eventual output\n",
    "        self.dropouti = dropouti  # Rate for dropping embedding output\n",
    "        self.dropoutl = dropoutl\n",
    "        self.var_drop = VariationalDropout()\n",
    "        \n",
    "        self.repack = repack\n",
    "        self.nout = nout          # Should be the number of classes\n",
    "        self.nhid = nhid\n",
    "        self.ninp = ninp\n",
    "        self.aux = aux\n",
    "        self.n_experts = n_experts\n",
    "        self.wnorm = wnorm\n",
    "\n",
    "        # Set up TrellisNet\n",
    "        tnet = TrellisNet\n",
    "        self.tnet = tnet(ninp, nhid, nout=nout, nlevels=nlevels, kernel_size=kernel_size,\n",
    "                         dropouth=dropouth, wnorm=wnorm, aux_frequency=aux_frequency, dilation=dilation)\n",
    "        \n",
    "        # Set up MoS, if needed\n",
    "        if n_experts > 0:\n",
    "            print(\"Applied Mixture of Softmax\")\n",
    "            self.mixsoft = MixSoftmax(n_experts, ntoken, nlasthid=nout, ninp=ninp, decoder=self.decoder,\n",
    "                                      dropoutl=dropoutl)\n",
    "            \n",
    "        # Apply weight drop connect. If weightnorm is used, we apply the dropout to its \"direction\", \n",
    "        # instead of \"scale\"\n",
    "        reg_term = '_v' if wnorm else ''\n",
    "        self.tnet = WeightDrop(self.tnet,[['full_conv', 'weight1' + reg_term],\n",
    "                                         ['full_conv', 'weight2' + reg_term]],\n",
    "                               dropout=wdrop, temporal=temporalwdrop)\n",
    "        self.linear = nn.Linear(nout, nout)\n",
    "        self.network = nn.ModuleList([self.tnet])\n",
    "        if n_experts > 0: self.network.append(self.mixsoft)\n",
    "\n",
    "            \n",
    "    def load_weights(self, params_dict):\n",
    "        self.load_state_dict(params_dict)\n",
    "\n",
    "        \n",
    "    def save_weights(self, name):\n",
    "        with open(name, 'wb') as f:\n",
    "            d = self.state_dict()\n",
    "            torch.save(d, f)\n",
    "            \n",
    "            \n",
    "    def forward(self, inputs, hidden):\n",
    "        # Dimension (N, C, L)\n",
    "        inputs = inputs.transpose(1, 2)\n",
    "        inputs = F.dropout(inputs, self.dropouti)\n",
    "        trellisnet = self.network[0]\n",
    "        raw_output, hidden, all_raw_outputs = trellisnet(inputs, hidden, aux=self.aux)\n",
    "        \n",
    "        if all_raw_outputs is not None:\n",
    "            all_raw_outputs = all_raw_outputs.transpose(2, 3)\n",
    "            \n",
    "        out = F.dropout(self.linear(raw_output), self.dropout)    # Dimension (N, n_classes)\n",
    "\n",
    "        return F.log_softmax(out, dim=2).transpose(1, 2), hidden   \n",
    "\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        h_size = self.nhid + self.nout\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(bsz, h_size, 1).zero_().to(device),\n",
    "                weight.new(bsz, h_size, 1).zero_().to(device))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight normalization applied\n",
      "Number of learnable Parameters: 237662\n"
     ]
    }
   ],
   "source": [
    "ninp = 120\n",
    "nhid = 120\n",
    "nout = 2\n",
    "nlevels = 11 \n",
    "kernel_size  = 2\n",
    "dilation = [1, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "dropout = 0.1\n",
    "dropouti = 0.1\n",
    "dropouth = 0.29\n",
    "dropoutl = 0.0\n",
    "emb_dropout = 0.02\n",
    "wdrop = 0.26\n",
    "temporalwdrop = True\n",
    "repack = False\n",
    "wnorm = True\n",
    "aux = 0\n",
    "aux_frequency = 1e4\n",
    "n_experts = 0\n",
    "    \n",
    "# wdecay = 8e-7\n",
    "# anneal = 5\n",
    "# seq_len = 0\n",
    "# horizon = 140\n",
    "# log-interval = 100\n",
    "# when = [220, 350]\n",
    "# n_experts = 0 \n",
    "\n",
    "model = TrellisNetModel(ninp, nhid, nout, nlevels, kernel_size, dilation, dropout, \n",
    "                        dropouti, dropouth, dropoutl, emb_dropout, wdrop, temporalwdrop, \n",
    "                        repack, wnorm, aux, aux_frequency, n_experts)\n",
    "\n",
    "print('Number of learnable Parameters: {}'.format(count_parameters(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 0.2\n",
    "\n",
    "loss_function = nn.NLLLoss(weight=torch.tensor([1., 70.], dtype=torch.float).to(device))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "train_loss_vec = []\n",
    "valid_loss_vec = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(0, 10000):  \n",
    "    for i, (feature, label) in enumerate(train_loader):\n",
    "        \n",
    "        # Clear out accumulates gradients \n",
    "        model.zero_grad()\n",
    "        \n",
    "        hidden = model.init_hidden(feature.size(0))\n",
    "\n",
    "        # Forward pass\n",
    "        out, _ = model(feature.to(device), hidden)\n",
    "\n",
    "        # Backward propagation\n",
    "        loss = loss_function(out, label.to(device))\n",
    "        train_loss_vec.append(loss.item())\n",
    "        loss.backward()    \n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print('Epoch: {:2d}   Batch: {:2d} of {:d}   TrainLoss: {:.3e}'\n",
    "        #       .format(epoch+1, i+1, len(train_loader), loss.item()))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        input, label = next(iter(valid_loader))\n",
    "\n",
    "        out, _ = model(input.to(device))\n",
    "        loss = loss_function(out, label.to(device))\n",
    "        valid_loss_vec.append(loss.item())\n",
    "\n",
    "        print('Epoch: {:2d}   Validation Loss: {:.3e}'\n",
    "                  .format(epoch+1, loss.item()))\n",
    "    \n",
    "    torch.save(model.state_dict(), '../models/lstm_ballroom_epoch'+ \n",
    "               str(epoch+1)+'_vloss_{:.5}.pt'.format(loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-698729ac181c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../models/lstm_ballroom_epoch'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_vloss_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), '../models/lstm_ballroom_epoch'+ str(epoch)+'_vloss_'+ str(loss.item()) +'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_loss_vec)\n",
    "plt.xlabel('Iteration steps')\n",
    "plt.ylabel('Loss');\n",
    "\n",
    "min_idx = np.argmin(valid_loss_vec)\n",
    "min_loss = np.min(valid_loss_vec)\n",
    "\n",
    "print('Minimum at {}: {}'.format(min_idx, min_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('valid_loss_8trellis_epochs', valid_loss_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss_vec = np.load('valid_loss_8trellis_epochs.npy').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('../models/mb_ballroom_bs-100_fold-0.pt', map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "403px",
    "left": "1064px",
    "right": "20px",
    "top": "114px",
    "width": "361px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
